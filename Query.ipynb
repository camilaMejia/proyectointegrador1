{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('estructuraDatos.sav', 'rb'))\n",
    "idexFiles = loaded_model['idexFiles']\n",
    "vectorizer = loaded_model['vectorizer']\n",
    "matrix = loaded_model['matriz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar los documentos que contiene una palabra en particular\n",
    "def encontrarDoc(palabra):\n",
    "    col = vectorizer.vocabulary_[palabra]\n",
    "    matx = matrix[:,col]\n",
    "    indx = matx.nonzero()[0]\n",
    "    lista =indx.tolist() \n",
    "    dfresult = pd.DataFrame()\n",
    "    for i in range(len(lista)):\n",
    "        auxres= pd.DataFrame({'NombreArchivo': idexFiles[lista[i]], 'Frecuencia': [matx.data[i]]})\n",
    "        dfresult = pd.concat([dfresult, auxres])\n",
    "    dfresult.sort_values('Frecuencia',ascending = False,inplace = True)\n",
    "    return dfresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indice_invertido(dic):\n",
    "    inv = {}\n",
    "    N = matrix.shape[0]\n",
    "    for k, v in vectorizer.vocabulary_.items():\n",
    "        inv.setdefault(k, {})\n",
    "        #Los documentos que contienen la palabra v\n",
    "        matx = matrix[:,v]\n",
    "        #Indicador de los documentos que contienen la palabra\n",
    "        indx = matx.nonzero()[0]\n",
    "        lista =indx.tolist()\n",
    "        docs = {}\n",
    "        if len(lista)== 0:\n",
    "            print(k)\n",
    "        else:\n",
    "            #Calculo del IDF, lista contiene todos los documentos que contienen la palabra\n",
    "            inv[k]['IDF'] = log((N+1)/(len(lista)))\n",
    "            for i in range(len(lista)):\n",
    "                keys = docs.setdefault(idexFiles[lista[i]], [])\n",
    "                #Frecuencia de la palabra V en el documento lista[i]\n",
    "                keys.append(matx.data[i])\n",
    "                #Las palabras que contiene el documento lista[i]\n",
    "                matx2 = matrix[lista[i],:]\n",
    "                #La frecuencia de cada palabra, que sumada el vector da el total de palabras en el documento\n",
    "                keys.append(matx2.data.sum())\n",
    "        inv[k]['Documentos'] = docs\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<979x1 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_['across']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "across\n",
      "all\n",
      "almost\n",
      "along\n",
      "also\n",
      "although\n",
      "am\n",
      "among\n",
      "amongst\n",
      "amount\n",
      "an\n",
      "and\n",
      "anyhow\n",
      "anyway\n",
      "around\n",
      "at\n",
      "back\n",
      "be\n",
      "beforehand\n",
      "behind\n",
      "between\n",
      "beyond\n",
      "bill\n",
      "both\n",
      "bottom\n",
      "call\n",
      "can\n",
      "cannot\n",
      "cant\n",
      "co\n",
      "con\n",
      "could\n",
      "de\n",
      "detail\n",
      "do\n",
      "down\n",
      "due\n",
      "eg\n",
      "eight\n",
      "either\n",
      "eleven\n",
      "enough\n",
      "etc\n",
      "even\n",
      "ever\n",
      "except\n",
      "fifteen\n",
      "fill\n",
      "find\n",
      "fire\n",
      "first\n",
      "five\n",
      "for\n",
      "former\n",
      "found\n",
      "four\n",
      "front\n",
      "full\n",
      "further\n",
      "get\n",
      "give\n",
      "go\n",
      "have\n",
      "he\n",
      "here\n",
      "herein\n",
      "how\n",
      "i\n",
      "ie\n",
      "if\n",
      "in\n",
      "inc\n",
      "interest\n",
      "it\n",
      "keep\n",
      "last\n",
      "latter\n",
      "least\n",
      "less\n",
      "ltd\n",
      "may\n",
      "me\n",
      "might\n",
      "mill\n",
      "mine\n",
      "move\n",
      "much\n",
      "must\n",
      "name\n",
      "neither\n",
      "never\n",
      "nevertheless\n",
      "next\n",
      "nine\n",
      "no\n",
      "none\n",
      "nor\n",
      "nothing\n",
      "off\n",
      "often\n",
      "on\n",
      "one\n",
      "onto\n",
      "or\n",
      "other\n",
      "out\n",
      "over\n",
      "own\n",
      "part\n",
      "per\n",
      "put\n",
      "rather\n",
      "re\n",
      "same\n",
      "see\n",
      "seem\n",
      "serious\n",
      "show\n",
      "side\n",
      "six\n",
      "so\n",
      "somehow\n",
      "still\n",
      "system\n",
      "take\n",
      "ten\n",
      "then\n",
      "therein\n",
      "these\n",
      "thick\n",
      "thin\n",
      "third\n",
      "though\n",
      "three\n",
      "throughout\n",
      "thru\n",
      "top\n",
      "toward\n",
      "two\n",
      "un\n",
      "under\n",
      "up\n",
      "upon\n",
      "us\n",
      "via\n",
      "well\n",
      "where\n",
      "wherein\n",
      "whereupon\n",
      "whether\n",
      "whoever\n",
      "whole\n",
      "whose\n",
      "will\n",
      "with\n",
      "within\n",
      "without\n",
      "would\n",
      "yet\n"
     ]
    }
   ],
   "source": [
    "ind_inv = indice_invertido(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "def queryClean(texto):\n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    texto =re.sub('(á|à|ä)','a',texto) # Reemplazar a acentuada\n",
    "    texto =re.sub('(é|è|ë)','e',texto) # Reemplazar e acentuada\n",
    "    texto =re.sub('(í|ì|ï)','i',texto) # Reemplazar i acentuada\n",
    "    texto =re.sub('(ó|ò|ö)','o',texto) # Reemplazar o acentuada\n",
    "    texto =re.sub('(ú|ù|ü)','u',texto) # Reemplazar u acentuada\n",
    "    texto =re.sub('[^a-zA-Z]',' ',texto) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    texto =re.sub(' +',' ',texto) # Eliminar espacios en blanco\n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    tokens = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    #Lemma\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "    tokens = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens]\n",
    "\n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens = [ps.stem(w) for w in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTF(word):\n",
    "    respuesta = sorted(ind_inv[word]['Documentos'].items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTFDL(word):\n",
    "    aux = ind_inv[word]['Documentos']\n",
    "    auxdic = {}\n",
    "    for k,v in aux.items():\n",
    "        keys = auxdic.setdefault(k, [])\n",
    "        keys.append(v[0]/v[1])\n",
    "    respuesta = sorted(auxdic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verquery = queryClean('intelligent')\n",
    "respuesta1 = queryTF(verquery[0])\n",
    "respuesta2 = queryTFDL(verquery[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encontrarDoc(verquery[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "respuesta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
