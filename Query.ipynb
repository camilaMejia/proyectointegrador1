{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\cmejia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import metapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el modelo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('estructuraDatos.sav', 'rb'))\n",
    "idexFiles = loaded_model['idexFiles']\n",
    "vectorizer = loaded_model['vectorizer']\n",
    "matrix = loaded_model['matriz']\n",
    "indexMeta = loaded_model['metapyIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankin solo por conteo sin indice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar los documentos que contiene una palabra en particular\n",
    "def encontrarDoc(palabra):\n",
    "    col = vectorizer.vocabulary_[palabra]\n",
    "    matx = matrix[:,col]\n",
    "    indx = matx.nonzero()[0]\n",
    "    lista =indx.tolist() \n",
    "    dfresult = pd.DataFrame()\n",
    "    for i in range(len(lista)):\n",
    "        auxres= pd.DataFrame({'NombreArchivo': idexFiles[lista[i]], 'Frecuencia': [matx.data[i]]})\n",
    "        dfresult = pd.concat([dfresult, auxres])\n",
    "    dfresult.sort_values('Frecuencia',ascending = False,inplace = True)\n",
    "    return dfresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción del índice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indice_invertido(dic):\n",
    "    inv = {}\n",
    "    N = matrix.shape[0]\n",
    "    for k, v in vectorizer.vocabulary_.items():\n",
    "        inv.setdefault(k, {})\n",
    "        #Los documentos que contienen la palabra v\n",
    "        matx = matrix[:,v]\n",
    "        #Indicador de los documentos que contienen la palabra\n",
    "        indx = matx.nonzero()[0]\n",
    "        lista =indx.tolist()\n",
    "        docs = {}\n",
    "        if len(lista)== 0:\n",
    "            print(k)\n",
    "        else:\n",
    "            #Calculo del IDF, lista contiene todos los documentos que contienen la palabra\n",
    "            inv[k]['IDF'] = log((N+1)/(len(lista)))\n",
    "            for i in range(len(lista)):\n",
    "                keys = docs.setdefault(idexFiles[lista[i]], [])\n",
    "                #Frecuencia de la palabra V en el documento lista[i]\n",
    "                keys.append(matx.data[i])\n",
    "                #Las palabras que contiene el documento lista[i]\n",
    "                matx2 = matrix[lista[i],:]\n",
    "                #La frecuencia de cada palabra, que sumada el vector da el total de palabras en el documento\n",
    "                keys.append(matx2.data.sum())\n",
    "        inv[k]['Documentos'] = docs\n",
    "    return inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "across\n",
      "all\n",
      "almost\n",
      "along\n",
      "also\n",
      "although\n",
      "am\n",
      "among\n",
      "amongst\n",
      "amount\n",
      "an\n",
      "and\n",
      "anyhow\n",
      "anyway\n",
      "around\n",
      "at\n",
      "back\n",
      "be\n",
      "beforehand\n",
      "behind\n",
      "between\n",
      "beyond\n",
      "bill\n",
      "both\n",
      "bottom\n",
      "call\n",
      "can\n",
      "cannot\n",
      "cant\n",
      "co\n",
      "con\n",
      "could\n",
      "de\n",
      "detail\n",
      "do\n",
      "down\n",
      "due\n",
      "eg\n",
      "eight\n",
      "either\n",
      "eleven\n",
      "enough\n",
      "etc\n",
      "even\n",
      "ever\n",
      "except\n",
      "fifteen\n",
      "fill\n",
      "find\n",
      "fire\n",
      "first\n",
      "five\n",
      "for\n",
      "former\n",
      "found\n",
      "four\n",
      "front\n",
      "full\n",
      "further\n",
      "get\n",
      "give\n",
      "go\n",
      "have\n",
      "he\n",
      "here\n",
      "herein\n",
      "how\n",
      "i\n",
      "ie\n",
      "if\n",
      "in\n",
      "inc\n",
      "interest\n",
      "it\n",
      "keep\n",
      "last\n",
      "latter\n",
      "least\n",
      "less\n",
      "ltd\n",
      "may\n",
      "me\n",
      "might\n",
      "mill\n",
      "mine\n",
      "move\n",
      "much\n",
      "must\n",
      "name\n",
      "neither\n",
      "never\n",
      "nevertheless\n",
      "next\n",
      "nine\n",
      "no\n",
      "none\n",
      "nor\n",
      "nothing\n",
      "off\n",
      "often\n",
      "on\n",
      "one\n",
      "onto\n",
      "or\n",
      "other\n",
      "out\n",
      "over\n",
      "own\n",
      "part\n",
      "per\n",
      "put\n",
      "rather\n",
      "re\n",
      "same\n",
      "see\n",
      "seem\n",
      "serious\n",
      "show\n",
      "side\n",
      "six\n",
      "so\n",
      "somehow\n",
      "still\n",
      "system\n",
      "take\n",
      "ten\n",
      "then\n",
      "therein\n",
      "these\n",
      "thick\n",
      "thin\n",
      "third\n",
      "though\n",
      "three\n",
      "throughout\n",
      "thru\n",
      "top\n",
      "toward\n",
      "two\n",
      "un\n",
      "under\n",
      "up\n",
      "upon\n",
      "us\n",
      "via\n",
      "well\n",
      "where\n",
      "wherein\n",
      "whereupon\n",
      "whether\n",
      "whoever\n",
      "whole\n",
      "whose\n",
      "will\n",
      "with\n",
      "within\n",
      "without\n",
      "would\n",
      "yet\n"
     ]
    }
   ],
   "source": [
    "ind_inv = indice_invertido(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de limpieza del query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')\n",
    "def queryClean(texto):\n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    texto =re.sub('(á|à|ä)','a',texto) # Reemplazar a acentuada\n",
    "    texto =re.sub('(é|è|ë)','e',texto) # Reemplazar e acentuada\n",
    "    texto =re.sub('(í|ì|ï)','i',texto) # Reemplazar i acentuada\n",
    "    texto =re.sub('(ó|ò|ö)','o',texto) # Reemplazar o acentuada\n",
    "    texto =re.sub('(ú|ù|ü)','u',texto) # Reemplazar u acentuada\n",
    "    texto =re.sub('[^a-zA-Z]',' ',texto) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    texto =re.sub(' +',' ',texto) # Eliminar espacios en blanco\n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    tokens = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    #Lemma\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "    tokens = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens]\n",
    "\n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens = [ps.stem(w) for w in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankin por Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTF(word,top):\n",
    "    respuesta = sorted(ind_inv[word]['Documentos'].items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "    return respuesta[0:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankin por Term Frequency / Doc Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryTFDL(word,top):\n",
    "    aux = ind_inv[word]['Documentos']\n",
    "    auxdic = {}\n",
    "    for k,v in aux.items():\n",
    "        keys = auxdic.setdefault(k, [])\n",
    "        keys.append(v[0]/v[1])\n",
    "    respuesta = sorted(auxdic.items(), key = lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "    return respuesta[0:top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankin usando BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_bm25(idf,frec,k,b,length,avgdl):\n",
    "    aux = idf*((frec*(k+1))/(frec+k*(1-b+b*length/avgdl)))\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryBM25(query, vocabulary, prom, k1, b, top):\n",
    "    query_word = queryClean(query)\n",
    "    dfresultb25 = pd.DataFrame()\n",
    "    resultadoBm25 = pd.DataFrame()\n",
    "    for word in query_word:\n",
    "        if (word in vocabulary):\n",
    "            aux = ind_inv[word]['Documentos']\n",
    "            IDF = ind_inv[word]['IDF']\n",
    "            for k,v in aux.items():\n",
    "            #     keys = bm25.setdefault(k, [])\n",
    "                aux25 = cal_bm25(IDF,v[0],k1,b,v[1],prom)\n",
    "                auxresb25= pd.DataFrame({'NombreArchivo': k.split('\\\\')[-1], 'Word': word, 'BM25' : [aux25]})\n",
    "                dfresultb25 = pd.concat([dfresultb25, auxresb25])\n",
    "            resultadoBm25 = dfresultb25.groupby('NombreArchivo').agg({'BM25':'sum'}).sort_values('BM25',ascending = False).reset_index()\n",
    "            resultadoBm25.reset_index(inplace = True)\n",
    "            resultadoBm25.rename(columns = {'index':'Ranking'}, inplace = True)\n",
    "        else:\n",
    "            print(f'{word} is not in the vocabulary')\n",
    "    return resultadoBm25.head(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de parámetros para el BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 20\n",
    "prom = 27544.226762002043\n",
    "k1 = 1.2\n",
    "b = 0.75\n",
    "vocabulary = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = metapy.index.make_inverted_index('cranfield.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos: 980\n",
      "Cantidad de palabras únicas: 51229\n",
      "Promedio de longitud de los documentos: 3984.62646484375\n"
     ]
    }
   ],
   "source": [
    "print(f'Total de documentos: {inv_idx.num_docs()}')\n",
    "print(f'Cantidad de palabras únicas: {inv_idx.unique_terms()}')\n",
    "print(f'Promedio de longitud de los documentos: {inv_idx.avg_doc_length()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metapy Rankin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankerMeta(top, querywords):\n",
    "    ranker = metapy.index.OkapiBM25(k1 = k1, b = b)\n",
    "    query = metapy.index.Document()\n",
    "    query.content(querywords) # query from AP news\n",
    "    top_docs = ranker.score(inv_idx, query, num_results=top)\n",
    "    metaresult = pd.DataFrame()\n",
    "    for doc in top_docs:\n",
    "        auxmeta= pd.DataFrame({'NombreArchivo': indexMeta[doc[0]],  'BM25_Meta' : [doc[1]]})\n",
    "        metaresult = pd.concat([metaresult, auxmeta])\n",
    "    metaresult = metaresult.reset_index(drop = True).reset_index()\n",
    "    metaresult.rename(columns = {'index':'RankingMeta'},inplace = True)\n",
    "    return metaresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de los queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sens(queries,top):\n",
    "    sens = pd.DataFrame()\n",
    "    for query in queries:\n",
    "        resultados = queryBM25(query, vocabulary, prom, k1, b,top)\n",
    "        metares = rankerMeta(top, query)\n",
    "        merget = resultados.merge(metares, how = 'left', on = 'NombreArchivo')\n",
    "        sensibilidad = (merget['RankingMeta']>=0).sum()/len(merget)\n",
    "        auxsens= pd.DataFrame({'Query': query,  'Sensibilidad' : [sensibilidad]})\n",
    "        sens = pd.concat([sens, auxsens])\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Data Science\",\"Machine Learning\", \"Math\",\"Computer Science\",\"Algorithms in dynamic networks\", \"triangle free process\"]\n",
    "sensibilidad = calculate_sens(queries,top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Sensibilidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Math</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computer Science</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algorithms in dynamic networks</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triangle free process</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Query  Sensibilidad\n",
       "0                    Data Science          0.75\n",
       "0                Machine Learning          0.95\n",
       "0                            Math          0.85\n",
       "0                Computer Science          0.65\n",
       "0  Algorithms in dynamic networks          0.90\n",
       "0           triangle free process          0.75"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = queryBM25(query, vocabulary, prom, k1, b,top)\n",
    "metares = rankerMeta(top, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranking</th>\n",
       "      <th>NombreArchivo</th>\n",
       "      <th>BM25</th>\n",
       "      <th>RankingMeta</th>\n",
       "      <th>BM25_Meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1509.02900.txt</td>\n",
       "      <td>2.283352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.236556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1503.06483.txt</td>\n",
       "      <td>2.277968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1506.00768.txt</td>\n",
       "      <td>2.264138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.210594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1402.6208.txt</td>\n",
       "      <td>2.259880</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.206688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1511.02476.txt</td>\n",
       "      <td>2.249759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1306.3261.txt</td>\n",
       "      <td>2.246745</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.160170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1411.7895.txt</td>\n",
       "      <td>2.244702</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.142693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1506.01634.txt</td>\n",
       "      <td>2.235767</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.094238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1408.0135.txt</td>\n",
       "      <td>2.235712</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.171219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1412.5902.txt</td>\n",
       "      <td>2.233992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.104872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1407.5117.txt</td>\n",
       "      <td>2.233979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.201272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1210.2246.txt</td>\n",
       "      <td>2.231717</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.180326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1411.4840.txt</td>\n",
       "      <td>2.228932</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.116774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1503.00244.txt</td>\n",
       "      <td>2.225537</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.135596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1506.00272.txt</td>\n",
       "      <td>2.225378</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.090242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1503.01239.txt</td>\n",
       "      <td>2.220727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1502.02511.txt</td>\n",
       "      <td>2.214410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1406.2587.txt</td>\n",
       "      <td>2.212030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1511.03518.txt</td>\n",
       "      <td>2.211633</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.120484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1406.5688.txt</td>\n",
       "      <td>2.208610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ranking   NombreArchivo      BM25  RankingMeta  BM25_Meta\n",
       "0         0  1509.02900.txt  2.283352          1.0   2.236556\n",
       "1         1  1503.06483.txt  2.277968          0.0   2.246560\n",
       "2         2  1506.00768.txt  2.264138          2.0   2.210594\n",
       "3         3   1402.6208.txt  2.259880          3.0   2.206688\n",
       "4         4  1511.02476.txt  2.249759          NaN        NaN\n",
       "5         5   1306.3261.txt  2.246745          7.0   2.160170\n",
       "6         6   1411.7895.txt  2.244702          8.0   2.142693\n",
       "7         7  1506.01634.txt  2.235767         16.0   2.094238\n",
       "8         8   1408.0135.txt  2.235712          6.0   2.171219\n",
       "9         9   1412.5902.txt  2.233992         14.0   2.104872\n",
       "10       10   1407.5117.txt  2.233979          4.0   2.201272\n",
       "11       11   1210.2246.txt  2.231717          5.0   2.180326\n",
       "12       12   1411.4840.txt  2.228932         12.0   2.116774\n",
       "13       13  1503.00244.txt  2.225537          9.0   2.135596\n",
       "14       14  1506.00272.txt  2.225378         19.0   2.090242\n",
       "15       15  1503.01239.txt  2.220727          NaN        NaN\n",
       "16       16  1502.02511.txt  2.214410          NaN        NaN\n",
       "17       17   1406.2587.txt  2.212030          NaN        NaN\n",
       "18       18  1511.03518.txt  2.211633         11.0   2.120484\n",
       "19       19   1406.5688.txt  2.208610          NaN        NaN"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados.merge(metares, how = 'left', on = 'NombreArchivo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15/20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
