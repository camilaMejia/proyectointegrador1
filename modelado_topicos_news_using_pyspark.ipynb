{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "#os.environ['PYSPARK_PYTHON'] = '/home/ubuntu/anaconda3/bin/python3'\n",
    "#os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/ubuntu/anaconda3/bin/ipython'\n",
    "#!pip install metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, isnan\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "import metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al ejecutar esta celda se demora un poco, así que un poco de paciencia\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local', \"news_cleaned_topic_model\") \n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "| id|id_news|               title|   publication|              author|      date|  year|month| url|             content|\n",
      "+---+-------+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "|  0|  17283|House Republicans...|New York Times|          Carl Hulse|2016-12-31|2016.0| 12.0|null|WASHINGTON  —   C...|\n",
      "|  1|  17284|Rift Between Offi...|New York Times|Benjamin Mueller ...|2017-06-19|2017.0|  6.0|null|After the bullet ...|\n",
      "|  2|  17285|Tyrus Wong, ‘Bamb...|New York Times|        Margalit Fox|2017-01-06|2017.0|  1.0|null|When Walt Disney’...|\n",
      "|  3|  17286|Among Deaths in 2...|New York Times|    William McDonald|2017-04-10|2017.0|  4.0|null|Death may be the ...|\n",
      "|  4|  17287|Kim Jong-un Says ...|New York Times|       Choe Sang-Hun|2017-01-02|2017.0|  1.0|null|SEOUL, South Kore...|\n",
      "|  5|  17288|Sick With a Cold,...|New York Times|         Sewell Chan|2017-01-02|2017.0|  1.0|null|LONDON  —   Queen...|\n",
      "|  6|  17289|Taiwan’s Presiden...|New York Times| Javier C. Hernández|2017-01-02|2017.0|  1.0|null|BEIJING  —   Pres...|\n",
      "|  7|  17290|After ‘The Bigges...|New York Times|         Gina Kolata|2017-02-08|2017.0|  2.0|null|Danny Cahill stoo...|\n",
      "|  8|  17291|First, a Mixtape....|New York Times|    Katherine Rosman|2016-12-31|2016.0| 12.0|null|Just how   is Hil...|\n",
      "|  9|  17292|Calling on Angels...|New York Times|         Andy Newman|2016-12-31|2016.0| 12.0|null|Angels are everyw...|\n",
      "+---+-------+--------------------+--------------+--------------------+----------+------+-----+----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNews=spark.read.csv(\"file:///home/ubuntu/Caso_estudio/datasets/news.csv\", inferSchema=True, header=True, encoding=\"UTF-8\")\n",
    "dfNews.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNewsFiltered=dfNews.na.drop(subset=(\"title\",\"publication\",\"author\",\"year\", \"date\", \"month\",\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, id_news: string, title: string, publication: string, author: string, date: string, year: string, month: string, url: string, content: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNewsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129519"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNewsFiltered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "dfStatistics =dfNewsFiltered.groupBy(\"publication\").agg(F.countDistinct(\"publication\"))\n",
    "#dfNewsFiltered.groupBy(\"publication\").count().orderBy().show()\n",
    "\n",
    "#dfNewsFiltered.registerTempTable(\"TblNews\")\n",
    "#dfStatistics = sqlContext.sql(\"select publication, year, count(distinct publication) from TblNews groupby publication\")\n",
    "#dfStatistics.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfStatistics.sort(desc(\"count(DISTINCT publication)\")\n",
    "\n",
    "#!rm -rf news-idx\n",
    "#idx = metapy.index.make_inverted_index('/home/ubuntu/bin/meta/build/config_news.toml')\n",
    "\n",
    "#print(f'Número Total de noticias: {idx.num_docs()}')\n",
    "#print(f'Número de palabras únicas: {idx.unique_terms()}')\n",
    "#print(f'Promedio de longitud de las documentos: {idx.avg_doc_length()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+--------------------+\n",
      "|id_news|               title|   publication|             content|\n",
      "+-------+--------------------+--------------+--------------------+\n",
      "|  17283|House Republicans...|New York Times|WASHINGTON  —   C...|\n",
      "|  17284|Rift Between Offi...|New York Times|After the bullet ...|\n",
      "|  17285|Tyrus Wong, ‘Bamb...|New York Times|When Walt Disney’...|\n",
      "|  17286|Among Deaths in 2...|New York Times|Death may be the ...|\n",
      "|  17287|Kim Jong-un Says ...|New York Times|SEOUL, South Kore...|\n",
      "|  17288|Sick With a Cold,...|New York Times|LONDON  —   Queen...|\n",
      "|  17289|Taiwan’s Presiden...|New York Times|BEIJING  —   Pres...|\n",
      "|  17290|After ‘The Bigges...|New York Times|Danny Cahill stoo...|\n",
      "|  17291|First, a Mixtape....|New York Times|Just how   is Hil...|\n",
      "|  17292|Calling on Angels...|New York Times|Angels are everyw...|\n",
      "+-------+--------------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMainCols= dfNewsFiltered.select('id_news','title','publication','content')\n",
    "dfMainCols.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "#from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publishers= dfMainCols.select(\"publication\").distinct()\n",
    "#publishers.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default list of Stopwords\n",
    "stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', \n",
    "                  u'are', u'arent', u'as', u'at', u'be', u'because', u'been', u'before', u'being', u'below', \n",
    "                  u'between', u'both', u'but', u'by',     u'can', 'cant', 'come', u'could', 'couldnt',u'd', u'did', \n",
    "                  u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', u'each', u'few', u'finally', \n",
    "                  u'for', u'from', u'further', u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', \n",
    "                  u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', u'i', u'if', u'in', u'into', \n",
    "                  u'is', u'isnt', u'it', u'its', u'itself', u'just', u'll',u'm', u'me', u'might', u'more', u'most', u'must',\n",
    "                  u'my', u'myself', u'no', u'nor', u'not', u'now', u'o', u'of', u'off', u'on', u'once', u'only', u'or', \n",
    "                  u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', u'r', u're', u's', 'said', u'same', \n",
    "                  u'she', u'should', u'shouldnt', u'so', u'some', u'such', u't', u'than', u'that', 'thats', u'the', u'their',\n",
    "                  u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', \n",
    "                  u'to', u'too', u'under', u'until', u'up',u'very', u'was', u'wasnt', u'we', u'were', u'werent', u'what', \n",
    "                  u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', u'y', \n",
    "                  u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "# Custom List of Stopwords - Add your own here\n",
    "stopwords_custom = ['New', 'York','Times','Breitbart', 'News']\n",
    "stopwords = stopwords_core + stopwords_custom\n",
    "stopwords = [word.lower() for word in stopwords] \n",
    "\n",
    "#Stemmer\n",
    "ps = PorterStemmer() \n",
    "\n",
    "def cleanup_text(record):\n",
    "    textTitle  = str(record[1])\n",
    "    textPublication=str(record[2])\n",
    "    textContent = str(record[3])\n",
    "    \n",
    "    #setStopwordsCustom =set(stopwords_custom)\n",
    "    #setStopwordsCustom =setStopwordsCustom.union(set(textPublication.split()))\n",
    "    #stopwords_custom1 = list(setStopwordsCustom)\n",
    "    #stopwords_custom= stopwords_custom1\n",
    "    #stopwords = stopwords_core + stopwords_custom1\n",
    "    #title=textTitle\n",
    "    #longPublication =len(textPublication) # Calcular la longitud de la cadena de caracteres  del publisher de la noticia\n",
    "    #longTitle=len(textTitle)  # Calcular la longitud de la cadena del titulo de la noticia\n",
    "    \n",
    "    #if longTitle > longPublication:\n",
    "    #    textPublisher=textTitle[longTitle-longPublication-1:longTitle] # Extrar caracteres derechos del titulo\n",
    "    #    if textPublisher==textPublication:\n",
    "    #        title=textTitle[0:longTitle-longPublication-1]\n",
    "    #    else:\n",
    "    #        title=textTitle\n",
    "    \n",
    "    textCombined = textTitle + \" \" + textContent\n",
    "    # Remove special characters\n",
    "    textCombined= re.sub('(-|-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015)\\n','',textCombined)\n",
    "    textCombined= re.sub('(-|-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015)',' ',textCombined)\n",
    "    textCombined= re.sub('[^(a-zA-Z)]',' ',textCombined)\n",
    "    textCombined= re.sub(' +',' ',textCombined)\n",
    "    words = textCombined.split()\n",
    "    tokens3 = [word.lower() for word in words if len(word)>3 and word.lower() not in stopwords] \n",
    "    text_out = [ps.stem(w) for w in tokens3]\n",
    "    \n",
    "    #text_out = [re.sub('(-|-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015)\\n','',word) for word in words]\n",
    "    #text_out = [re.sub('(-|-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015)',' ',word) for word in words]\n",
    "    #text_out = [re.sub('[^(a-zA-Z)]',' ',word) for word in words]                                   \n",
    "    #text_out = [re.sub(' +',' ',word) for word in words] \n",
    "    # Remove stopwords and words under X length\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "dfCombined = dfMainCols.withColumn(\"CombinedTokens\", udf_cleantext(struct([dfMainCols[x] for x in dfMainCols.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------+--------------------+--------------------+\n",
      "|id_news|               title|   publication|             content|      CombinedTokens|\n",
      "+-------+--------------------+--------------+--------------------+--------------------+\n",
      "|  17283|House Republicans...|New York Times|WASHINGTON  —   C...|[hous, republican...|\n",
      "|  17284|Rift Between Offi...|New York Times|After the bullet ...|[rift, offic, res...|\n",
      "|  17285|Tyrus Wong, ‘Bamb...|New York Times|When Walt Disney’...|[tyru, wong, bamb...|\n",
      "|  17286|Among Deaths in 2...|New York Times|Death may be the ...|[among, death, he...|\n",
      "|  17287|Kim Jong-un Says ...|New York Times|SEOUL, South Kore...|[jong, say, north...|\n",
      "|  17288|Sick With a Cold,...|New York Times|LONDON  —   Queen...|[sick, cold, quee...|\n",
      "|  17289|Taiwan’s Presiden...|New York Times|BEIJING  —   Pres...|[taiwan, presid, ...|\n",
      "|  17290|After ‘The Bigges...|New York Times|Danny Cahill stoo...|[biggest, loser, ...|\n",
      "|  17291|First, a Mixtape....|New York Times|Just how   is Hil...|[first, mixtap, r...|\n",
      "|  17292|Calling on Angels...|New York Times|Angels are everyw...|[call, angel, end...|\n",
      "+-------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCombined.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-356693d22b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Term Frequency Vectorization:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcvTokensCombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CombinedTokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcvmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvTokensCombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfCombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeaturizedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfCombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Term Frequency Vectorization: \n",
    "cvTokensCombined = CountVectorizer(inputCol=\"CombinedTokens\", outputCol=\"rawFeatures\", vocabSize = 500)\n",
    "cvmodel = cvTokensCombined.fit(dfCombined)\n",
    "featurizedData = cvmodel.transform(dfCombined)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "#Calculating IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 20 Data-Driven Topics:\n",
    "lda = LDA(k=20, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "ldatopics.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizedData.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('id_news','title','publication','content','CombinedTokens','features','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults.select('id_news','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults.topicDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "def asignTopic(topicDistribution):\n",
    "    topic = topicDistribution[0]\n",
    "    index_topic = 0\n",
    "    for index in range(len(topicDistribution)):\n",
    "        if (topicDistribution[index]>topic):\n",
    "            topic=topicDistribution[index]\n",
    "            index_topic=index\n",
    "    return index_topic\n",
    "\n",
    "udf_asignTopic = udf(asignTopic, IntegerType())\n",
    "dfMainTopic = ldaResults.withColumn(\"mainTopic\", udf_asignTopic(ldaResults.topicDistribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNewsTopic.select('mainTopic').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import StructType,ArrayType,StringType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=pd.read_csv(\"file:///home/ubuntu/Caso_estudio/datasets/mini_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n",
    "Se elimina del titulo la publicación, esto ayuda a mejorar la clasifiación de los sentimientos de la noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat=dat[dat['title'].fillna(0, inplace=True)]\n",
    "zipped = zip(dat['title'], dat['content'])\n",
    "title=[]\n",
    "content=[]\n",
    "sep = ' - '\n",
    "for i,j in zipped:\n",
    "    if pd.notna(i):\n",
    "        tit=i\n",
    "        p = tit.split(sep, 1)[0]\n",
    "        content.append(j)\n",
    "        title.append(p)\n",
    "    else:\n",
    "        title.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificación del sentimiento\n",
    "0: negativo,\n",
    "1: neutro,\n",
    "2: positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=[]\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for t in title:\n",
    "    comp=sid.polarity_scores(t)['compound'] # Score de sentimiento\n",
    "    #print(comp)\n",
    "    if comp<0:\n",
    "        lb=0\n",
    "    elif comp==0:\n",
    "        lb=1\n",
    "    else:\n",
    "        lb=2\n",
    "    target.append(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'title':title,'content':content,'target':target}\n",
    "dfsent = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sick With a Cold, Queen Elizabeth Misses New Y...</td>\n",
       "      <td>LONDON  —   Queen Elizabeth II, who has been b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taiwan’s President Accuses China of Renewed In...</td>\n",
       "      <td>BEIJING  —   President Tsai   of Taiwan sharpl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calling on Angels While Enduring the Trials of...</td>\n",
       "      <td>Angels are everywhere in the Muñiz family’s ap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  House Republicans Fret About Winning Their Hea...   \n",
       "1  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "2  Sick With a Cold, Queen Elizabeth Misses New Y...   \n",
       "3  Taiwan’s President Accuses China of Renewed In...   \n",
       "4  Calling on Angels While Enduring the Trials of...   \n",
       "\n",
       "                                             content  target  \n",
       "0  WASHINGTON  —   Congressional Republicans have...       2  \n",
       "1  SEOUL, South Korea  —   North Korea’s leader, ...       1  \n",
       "2  LONDON  —   Queen Elizabeth II, who has been b...       0  \n",
       "3  BEIJING  —   President Tsai   of Taiwan sharpl...       0  \n",
       "4  Angels are everywhere in the Muñiz family’s ap...       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsent.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_dfsent = sqlContext.createDataFrame(dfsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|               title|             content|target|\n",
      "+--------------------+--------------------+------+\n",
      "|House Republicans...|WASHINGTON  —   C...|     2|\n",
      "|Kim Jong-un Says ...|SEOUL, South Kore...|     1|\n",
      "|Sick With a Cold,...|LONDON  —   Queen...|     0|\n",
      "|Taiwan’s Presiden...|BEIJING  —   Pres...|     0|\n",
      "|Calling on Angels...|Angels are everyw...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dfsent.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                text|target|\n",
      "+--------------------+------+\n",
      "|washington     co...|     2|\n",
      "|seoul south korea...|     1|\n",
      "|london     queen ...|     0|\n",
      "|beijing     presi...|     0|\n",
      "|angels are everyw...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean=spark_dfsent.select(lower(regexp_replace(spark_dfsent.content, \"[^a-zA-Z\\\\s]\", \"\")).alias('text'),spark_dfsent.target)\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                text|         words_token|target|\n",
      "+--------------------+--------------------+------+\n",
      "|washington     co...|[washington, , , ...|     2|\n",
      "|seoul south korea...|[seoul, south, ko...|     1|\n",
      "|london     queen ...|[london, , , , , ...|     0|\n",
      "|beijing     presi...|[beijing, , , , ,...|     0|\n",
      "|angels are everyw...|[angels, are, eve...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='words_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select('text', 'words_token','target')\n",
    "df_words_token.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|         words_token|         words_clean|target|\n",
      "+--------------------+--------------------+------+\n",
      "|[washington, , , ...|[washington, , , ...|     2|\n",
      "|[seoul, south, ko...|[seoul, south, ko...|     1|\n",
      "|[london, , , , , ...|[london, , , , , ...|     0|\n",
      "|[beijing, , , , ,...|[beijing, , , , ,...|     0|\n",
      "|[angels, are, eve...|[angels, everywhe...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "df_words_no_stopw = remover.transform(df_words_token).select('words_token', 'words_clean','target')\n",
    "df_words_no_stopw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset: 168\n",
      "Test Dataset: 81\n"
     ]
    }
   ],
   "source": [
    "train, test = df_clean.randomSplit([0.7, 0.3], seed = 2019)\n",
    "print(\"Training Dataset: \" + str(train.count()))\n",
    "print(\"Test Dataset: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparacióm de datos co Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(train)\n",
    "train_df = pipelineFit.transform(train)\n",
    "val_df = pipelineFit.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|                text|target|               words|                  tf|            features|label|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|  donald j trump ...|     0|[, , donald, j, t...|(65536,[65,88,556...|(65536,[65,88,556...|  1.0|\n",
      "|  donald j trump ...|     0|[, , donald, j, t...|(65536,[187,389,5...|(65536,[187,389,5...|  1.0|\n",
      "| want to get this...|     2|[, want, to, get,...|(65536,[12,110,12...|(65536,[12,110,12...|  2.0|\n",
      "|a controversial e...|     2|[a, controversial...|(65536,[61,338,49...|(65536,[61,338,49...|  2.0|\n",
      "|a few hours after...|     1|[a, few, hours, a...|(65536,[14,150,55...|(65536,[14,150,55...|  0.0|\n",
      "|a florida man has...|     0|[a, florida, man,...|(65536,[308,338,5...|(65536,[308,338,5...|  1.0|\n",
      "|a long island rai...|     1|[a, long, island,...|(65536,[209,368,3...|(65536,[209,368,3...|  0.0|\n",
      "|a man who admitte...|     0|[a, man, who, adm...|(65536,[1038,1444...|(65536,[1038,1444...|  1.0|\n",
      "|a steep drop in g...|     0|[a, steep, drop, ...|(65536,[264,571,1...|(65536,[264,571,1...|  1.0|\n",
      "|after  years of s...|     1|[after, , years, ...|(65536,[168,312,6...|(65536,[168,312,6...|  0.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RadomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "pipelineFit = pipeline.fit(train)\n",
    "train_df = pipelineFit.transform(train)\n",
    "test_df = pipelineFit.transform(test)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train_df)\n",
    "predictions_Forest = rfModel.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|label|       rawPrediction|prediction|         probability|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|  0.0|[11.0932481879091...|       0.0|[0.55466240939545...|\n",
      "|  0.0|[8.97017324623525...|       0.0|[0.44850866231176...|\n",
      "|  0.0|[10.4919579134395...|       0.0|[0.52459789567197...|\n",
      "|  1.0|[9.79037011562038...|       0.0|[0.48951850578101...|\n",
      "|  2.0|[9.22143251870938...|       0.0|[0.46107162593546...|\n",
      "|  1.0|[11.4882028182988...|       0.0|[0.57441014091494...|\n",
      "|  1.0|[10.6830839819691...|       0.0|[0.53415419909845...|\n",
      "|  2.0|[10.2451587222944...|       0.0|[0.51225793611472...|\n",
      "|  1.0|[9.89556686359638...|       0.0|[0.49477834317981...|\n",
      "|  1.0|[8.82958527230802...|       0.0|[0.44147926361540...|\n",
      "|  1.0|[10.7393600937554...|       0.0|[0.53696800468777...|\n",
      "|  1.0|[9.05098050683347...|       0.0|[0.45254902534167...|\n",
      "|  1.0|[11.3812948512002...|       0.0|[0.56906474256001...|\n",
      "|  0.0|[9.08637573344374...|       0.0|[0.45431878667218...|\n",
      "|  1.0|[10.0717235375923...|       0.0|[0.50358617687961...|\n",
      "|  1.0|[9.56959631824061...|       0.0|[0.47847981591203...|\n",
      "|  0.0|[10.7944745023109...|       0.0|[0.53972372511554...|\n",
      "|  1.0|[8.56676424984901...|       0.0|[0.42833821249245...|\n",
      "|  0.0|[11.8309249071034...|       0.0|[0.59154624535517...|\n",
      "|  0.0|[10.4667639144866...|       0.0|[0.52333819572433...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_Forest.select('label', 'rawPrediction', 'prediction', 'probability').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5499999999999999\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions_Forest, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.maxBins, [20, 60])\n",
    "             .addGrid(rf.numTrees, [5, 20])\n",
    "             .build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4925925925925926"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Best model's predictions and probabilities of each prediction class\n",
    "#selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "#display(selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+----------+--------------------+\n",
      "|target|label|       rawPrediction|prediction|         probability|\n",
      "+------+-----+--------------------+----------+--------------------+\n",
      "|     1|  0.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     1|  0.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     1|  0.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     0|  1.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     2|  2.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     0|  1.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     0|  1.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     2|  2.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     0|  1.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "|     0|  1.0|[0.45371872668574...|       0.0|[0.48214244232133...|\n",
      "+------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "mlr = LogisticRegression(featuresCol = 'features', labelCol = 'label',maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "mlrModel = mlr.fit(train_df)\n",
    "mlpredictions = mlrModel.transform(test_df)\n",
    "mlpredictions.select('target','label', 'rawPrediction', 'prediction', 'probability').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(mlpredictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
