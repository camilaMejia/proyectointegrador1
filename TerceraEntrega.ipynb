{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de columnas es: 5\n",
      "Las candidatas a ser seleccionada para un procesamiento de texto serían: title, description y subject\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>subject</th>\n",
       "      <th>creator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/0704.3504</td>\n",
       "      <td>Smooth R\\'enyi Entropy of Ergodic Quantum Info...</td>\n",
       "      <td>We prove that the average smooth Renyi entro...</td>\n",
       "      <td>Quantum Physics ; Computer Science - Informati...</td>\n",
       "      <td>Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/0706.1402</td>\n",
       "      <td>Analyzing Design Process and Experiments on th...</td>\n",
       "      <td>In the field of tutoring systems, investigat...</td>\n",
       "      <td>Computer Science - Computers and Society ; Com...</td>\n",
       "      <td>Brust, Matthias R. ; Rothkugel, Steffen ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/0710.0736</td>\n",
       "      <td>Colour image segmentation by the vector-valued...</td>\n",
       "      <td>We propose a new method for the numerical so...</td>\n",
       "      <td>Computer Science - Computer Vision and Pattern...</td>\n",
       "      <td>Kay, David A ; Tomasi, Alessandro ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/0803.2570</td>\n",
       "      <td>Unequal Error Protection: An Information Theor...</td>\n",
       "      <td>An information theoretic framework for unequ...</td>\n",
       "      <td>Computer Science - Information Theory ; Comput...</td>\n",
       "      <td>Borade, Shashi ; Nakiboglu, Baris ; Zheng, Liz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/0808.0084</td>\n",
       "      <td>On the hitting times of quantum versus random ...</td>\n",
       "      <td>In this paper we define new Monte Carlo type...</td>\n",
       "      <td>Quantum Physics ; Computer Science - Data Stru...</td>\n",
       "      <td>Magniez, Frederic ; Nayak, Ashwin ; Richter, P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       identifier  \\\n",
       "0  http://arxiv.org/abs/0704.3504   \n",
       "1  http://arxiv.org/abs/0706.1402   \n",
       "2  http://arxiv.org/abs/0710.0736   \n",
       "3  http://arxiv.org/abs/0803.2570   \n",
       "4  http://arxiv.org/abs/0808.0084   \n",
       "\n",
       "                                               title  \\\n",
       "0  Smooth R\\'enyi Entropy of Ergodic Quantum Info...   \n",
       "1  Analyzing Design Process and Experiments on th...   \n",
       "2  Colour image segmentation by the vector-valued...   \n",
       "3  Unequal Error Protection: An Information Theor...   \n",
       "4  On the hitting times of quantum versus random ...   \n",
       "\n",
       "                                         description  \\\n",
       "0    We prove that the average smooth Renyi entro...   \n",
       "1    In the field of tutoring systems, investigat...   \n",
       "2    We propose a new method for the numerical so...   \n",
       "3    An information theoretic framework for unequ...   \n",
       "4    In this paper we define new Monte Carlo type...   \n",
       "\n",
       "                                             subject  \\\n",
       "0  Quantum Physics ; Computer Science - Informati...   \n",
       "1  Computer Science - Computers and Society ; Com...   \n",
       "2  Computer Science - Computer Vision and Pattern...   \n",
       "3  Computer Science - Information Theory ; Comput...   \n",
       "4  Quantum Physics ; Computer Science - Data Stru...   \n",
       "\n",
       "                                             creator  \n",
       "0  Schoenmakers, Berry ; Tjoelker, Jilles ; Tuyls...  \n",
       "1         Brust, Matthias R. ; Rothkugel, Steffen ;   \n",
       "2               Kay, David A ; Tomasi, Alessandro ;   \n",
       "3  Borade, Shashi ; Nakiboglu, Baris ; Zheng, Liz...  \n",
       "4  Magniez, Frederic ; Nayak, Ashwin ; Richter, P...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('data/articles.csv',sep = ',', encoding= 'utf-8')\n",
    "print(f\"La cantidad de columnas es: {len(articles.columns)}\")\n",
    "print(f\"Las candidatas a ser seleccionada para un procesamiento de texto serían: title, description y subject\")\n",
    "articles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles['subject_split'] = articles['subject'].str.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject          Computer Science - Computers and Society ; Com...\n",
       "subject_split    [Computer Science - Computers and Society ,  C...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[['subject','subject_split']] .iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['combine_column'] = articles['title']+articles['description']\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens inicial: 185949\n",
      "Cantidad de tokens después de remover lo que no sea letras y que la longitud sea 1: 151449\n",
      "Cantidad de tokens después de remover las stop words: 93892\n"
     ]
    }
   ],
   "source": [
    "process_data3 = articles[['identifier','combine_column']].copy()\n",
    "\n",
    "process_data3['combine_column']  = process_data3['combine_column'] .str.lower()\n",
    "\n",
    "process_data3['tokens'] = process_data3['combine_column'].apply(lambda x: word_tokenize(x))\n",
    "#process_data3['tokens2'] = process_data3['combine_column'].apply(lambda x: RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+').tokenize(x))\n",
    "process_data3['len_ini'] = process_data3['tokens'].apply(lambda x: len(x))\n",
    "\n",
    "process_data3['clean_tokens'] = process_data3['tokens'].apply(lambda x:[re.sub('[^a-zA-Z]','',item) for item in x])\n",
    "process_data3['clean_tokens'] = process_data3['clean_tokens'].apply(lambda x: [w for w in x if (len(w)>1)&(w.isalpha())])\n",
    "process_data3['len_med'] = process_data3['clean_tokens'].apply(lambda x: len(x))\n",
    "process_data3['clean_tokens'] = process_data3['clean_tokens'].apply(lambda x: [w for w in x if (w not in stopWords)])\n",
    "process_data3['len_no_stop_words'] = process_data3['clean_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "print(f\"Cantidad de tokens inicial: {process_data3['len_ini'].sum()}\")\n",
    "print(f\"Cantidad de tokens después de remover lo que no sea letras y que la longitud sea 1: {process_data3['len_med'].sum()}\")\n",
    "print(f\"Cantidad de tokens después de remover las stop words: {process_data3['len_no_stop_words'].sum()}\")\n",
    "\n",
    "vocabulary3 = set()\n",
    "for word in process_data3['clean_tokens']:\n",
    "    vocabulary3 = vocabulary3.union(set(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.111690s\n",
      "n_samples: 980, n_features: 5225\n",
      "\n",
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "        init_size=1000, max_iter=100, max_no_improvement=10, n_clusters=5,\n",
      "        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=False)\n",
      "done in 0.197s\n",
      "\n",
      "Silhouette Coefficient: 0.004\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: codes optimization energy channel signal sensor power networks sensing proposed\n",
      "Cluster 1: information logic paper systems linear networks based results properties social\n",
      "Cluster 2: data learning model method based methods models network matrix using\n",
      "Cluster 3: quantum classical bounds proof lower complexity proofs realizability protocols reversible\n",
      "Cluster 4: graph graphs algorithm time problem log number tree algorithms edge\n"
     ]
    }
   ],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Total of categories\n",
    "true_k = 5\n",
    "\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None, binary=False)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2',\n",
    "                                       binary=False)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(process_data3['combine_column'].values)\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=opts.verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "# print(\"Adjusted Rand-Index: %.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
