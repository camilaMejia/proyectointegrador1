{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_file_path es la ruta donde se encuentran los archivos de texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  Path(\"~\").expanduser().resolve()\n",
    "#base_path = Path.cwd().expanduser().resolve()\n",
    "input_file_path  = base_path / 'datasets/papers-txt/'\n",
    "#datasetOut =base_path / \"datasets/salidas_procesamiento/\"\n",
    "#datasetOut_freq = base_path / \"datasets/salidas_freq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cmejia3/datasets/papers-txt')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileSize(fileIn):\n",
    "    size=os.stat(fileIn).st_size\n",
    "    return size\n",
    "\n",
    "def wordsCount(contenido):\n",
    "    totWwords=contenido.split()\n",
    "    return len(totWwords)\n",
    "\n",
    "def cleanWordCount(contenido):\n",
    "    contenido =re.sub('((f|ht)tp(s?)://)?(.*)[.][a-z]+(((\\/.*(\\/?)){1,}?)(.*([.].*)?))?',' ',contenido) # Eliminar las URL\n",
    "    #contenido =re.sub('REFERENCES (\\S|\\w)+',' ',contenido) # Eliminar la bibliografia\n",
    "    contenido =re.sub('[a-zA-Z0-9.?{}]+@\\w+\\.\\w+.\\w*','',contenido) # Eliminar los correos\n",
    "    contenido =re.sub('\\[[a-zA-Z0-9\\,\\. ]+\\]','',contenido) # Eliminar cualquier contenido entre corchetes\n",
    "    contenido =re.sub('\\([a-zA-Z0-9\\,\\.\\- ]+\\)',' ',contenido) # Eliminar cualquier contenido entre paréntesis\n",
    "    contenido =re.sub('((et al\\.)|(i\\.i\\.d\\.)|(i\\.e\\.)|\\'|\\’|\\`)','',contenido) # Eliminar abreviaciones, apostrofes y guion\n",
    "    contenido =re.sub('(á|à|ä)','a',contenido) # Reemplazar a acentuada\n",
    "    contenido =re.sub('(é|è|ë)','e',contenido) # Reemplazar e acentuada\n",
    "    contenido =re.sub('(í|ì|ï)','i',contenido) # Reemplazar i acentuada\n",
    "    contenido =re.sub('(ó|ò|ö)','o',contenido) # Reemplazar o acentuada\n",
    "    contenido =re.sub('(ú|ù|ü)','u',contenido) # Reemplazar u acentuada\n",
    "    contenido =re.sub('[^a-zA-Z_|\\-\\s]',' ',contenido) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    contenido =re.sub(' +',' ',contenido) # Eliminar espacios en blanco\n",
    "    contenido =re.sub('(a-z|A-Z){1,1}','',contenido) # Eliminar palabras o números de un caracter de longitud  \n",
    "    totWwords=contenido.split()\n",
    "    setWords = set(totWwords)\n",
    "    #print(\"Total de palabras {}\".format(len(totWwords)))\n",
    "    #print(\"Total de palabras después del pre-procesamiento: {}\".format(totWordDepurado))\n",
    "    return len(totWwords),contenido,setWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_ratio(input):\n",
    "    lang_ratio = {}\n",
    "    tokens = wordpunct_tokenize(input)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        word_set = set(words)\n",
    "        common_elements = word_set.intersection(stopwords_set)\n",
    "        lang_ratio[language] = len(common_elements)\n",
    "    return lang_ratio\n",
    "\n",
    "def detect_language(input):\n",
    "    ratios = lang_ratio(input)\n",
    "    language = max(ratios, key = ratios.get)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de las stop words que serán eliminadas ya que no aportan valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras que considermos StopWords que no estan incluidas en el conjunto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoStopWords = ['www','https','html','figure', 'chapter','abbcbccabcabcabcabcbcbabacba']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añaden las palabras que consideramos al conjunto principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.extend(listoStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función principal procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(texto,stopWords):\n",
    "    \n",
    "    #Quitar todos los acentos\n",
    "    #texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    #Quitar todos los caracteres especiales\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto)\n",
    "    \n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    #Variable que guarda el año en el que estamos que es el limite superior de los números que no se van a eliminar\n",
    "    currentYear = int(dt.datetime.now().year)\n",
    "    \n",
    "    #Verificar que las palabras tengan más de un caracter, que además sean solo sean letras\n",
    "    # o si son números que esten entre un rango que sea admisible para no eliminar información de año que se mencione en los artículos\n",
    "    # y finalmente que no sean palabras que estan en el dicccionario de stopwords.\n",
    "    \n",
    "    #tokens = [w for w in tokens if (len(w)>1)&(w.isalpha() or (w.isnumeric() and int(w)>=1800 and int(w)<=currentYear))&(w not in stopWords)]\n",
    "    tokens = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    \n",
    "    matcher= re.compile(r'(.)\\1*')\n",
    "    tokens1 = []\n",
    "    for word in tokens:\n",
    "        aux = [len(match.group()) for match in matcher.finditer(word)]\n",
    "        if max(aux)<=3:\n",
    "            tokens1.append(word)\n",
    "        else:\n",
    "            print(word)\n",
    "    \n",
    "    \n",
    "    #tokens1 = [w for w in tokens1 if (Counter(w).most_common(1)[0][1]<5)]\n",
    "    #eliminadas = [w for w in tokens1 if (Counter(w).most_common(1)[0][1]>=5)]\n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens1 = [ps.stem(w) for w in tokens1]\n",
    "    \n",
    "    #Lematización\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "\n",
    "    tokens1 = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens1]\n",
    "    \n",
    "    #Se retorna el texto nuevamente en un solo string luego de ser procesado\n",
    "    to_return = ' '.join(tokens1)\n",
    "    \n",
    "    #Se retorna el vocabulario de cada documento\n",
    "    set_words = set(tokens1)\n",
    "    \n",
    "    #Y la frecuencia de las palabras\n",
    "    freq = nltk.FreqDist(tokens1)\n",
    "    return to_return,set_words,freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de los conjuntos:\n",
    "\n",
    "    - Vocabulary: el conjunto de todas las palabras que contienen los documentos\n",
    "    - results_text: la lista con los documentos ya organizados para construir el bag of words\n",
    "    - results_frecuency: información de cada documento de las palabras que contiene cuántas veces las contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "results_text = []\n",
    "results_frecuency = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxx\n",
      "vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "xxxxxxx\n",
      "xxxxxxx\n",
      "aaaa\n",
      "aaaaa\n",
      "aaaaaa\n",
      "baaaaa\n",
      "yyyy\n",
      "yyyy\n",
      "xxxxxxx\n",
      "yyyyyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "cacgcaccccctcg\n",
      "cacgcaccccctcg\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "xxxxx\n",
      "xxxxx\n",
      "looooooooooooooooooooooooooooooomooooooooooooooooooooooooooooooon\n",
      "looooooooooooomooooooooooooon\n",
      "loooooooooooooomoooooooooooooon\n",
      "aaaab\n",
      "german:/home/cmejia3/datasets/papers-txt/1508.02340.txt\n",
      "xxxx\n",
      "yyyy\n",
      "oooo\n",
      "xxxxx\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "pppperspectives\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "xxxx\n",
      "xxxx\n",
      "ffff\n",
      "fffff\n",
      "ffff\n",
      "ffff\n",
      "ffff\n",
      "xxxxxxxx\n",
      "xxxxx\n",
      "xxxx\n",
      "xxxxx\n",
      "fffff\n",
      "ffff\n",
      "ffffff\n",
      "yyyy\n",
      "eeee\n",
      "bbbbbb\n",
      "bbbbbb\n",
      "bbbbbb\n",
      "bbbbbb\n",
      "bbbbbb\n",
      "bbbbbb\n",
      "bsbbbbbbbbbbb\n",
      "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "ssss\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "xxxxx\n",
      "rrrr\n",
      "rrrr\n",
      "rrrr\n",
      "rrrr\n",
      "xxxx\n",
      "xxxx\n",
      "ababaaaaaaabaaaaabababaaaaab\n",
      "aaaaaaaaaaaaaaaababa\n",
      "ssss\n",
      "ssss\n",
      "ppppp\n",
      "pppp\n",
      "ppppp\n",
      "pppp\n",
      "xxxx\n",
      "xxxx\n",
      "xxxx\n",
      "hkkkkkkkkkkkikkkkkkkkkkkj\n",
      "bbaaabbccccaabbbaa\n",
      "aaabbbbaaccccaaaacbbbcccbb\n",
      "ieeee\n",
      "xxxx\n",
      "xxxx\n",
      "xxxxxxx\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "xxxxx\n",
      "bbbb\n",
      "xffff\n",
      "xffff\n",
      "xffffffffu\n",
      "xffffffffffffffffu\n",
      "xffffffffu\n",
      "mjqxxxx\n",
      "aaaa\n",
      "aaaaaa\n",
      "aaaaaa\n",
      "qqqq\n",
      "qqqq\n",
      "qqqq\n",
      "xxxx\n",
      "xxxx\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "aaaa\n",
      "cooooool\n",
      "xxxx\n",
      "yssss\n",
      "xffffffffl\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "yyyy\n",
      "ofaaaa\n",
      "aaaaaaa\n",
      "aaaa\n"
     ]
    }
   ],
   "source": [
    "fileSummary = \"CleanSummary.csv\"\n",
    "\n",
    "contenido= \"Archivo\" + \";\" + \"Tamaño(K)\" + \";\" + \"Cant Palabras Inicial\" + \";\" + \"Cant Palabras depuradas\"+ \";\" +\"Porc Limpieza\"+ \"\\n\"\n",
    "resultado = pd.DataFrame()\n",
    "indexFiles = []\n",
    "documents = []\n",
    "\n",
    "for f in input_file_path.glob('*.txt'):\n",
    "    #Peso del archivo\n",
    "    tmpSize=round(fileSize(f)/1014)\n",
    "    \n",
    "    #Lectura del archivo\n",
    "    input_file = open(f, \"r\", encoding = 'utf-8')\n",
    "    input_aux = input_file.read()\n",
    "    \n",
    "    #Cuenta de palabras iniciales\n",
    "    tmpWordsOri=wordsCount(input_aux)\n",
    "    #out = datasetOut / str(f).split('/')[-1]\n",
    "    \n",
    "    #Cuenta de palabras despues de la limpieza\n",
    "    tmpWordsEnd,text,setWord=cleanWordCount(input_aux)\n",
    "    #tmpPerClean=round((tmpWordsEnd/tmpWordsOri)*100)\n",
    "    \n",
    "    \n",
    "    #Detectando el idioma\n",
    "    aux = detect_language(text)\n",
    "    \n",
    "    if(aux == 'english'):\n",
    "        text_cleanned,set_words,freq = clean_files(text,stopWords)\n",
    "        #out2 = datasetOut / str(f).split('/')[-1]\n",
    "        documents.append(text_cleanned)\n",
    "        vocabulary = vocabulary.union(set_words)\n",
    "        results_text.append(text_cleanned)\n",
    "        indexFiles.append(str(f).split('/')[-1])\n",
    "        \n",
    "        #Escritura de los resultados del preprocesamiento\n",
    "        auxRes= pd.DataFrame({'Archivo': str(f).split('/')[-1], 'Tamaño(K)': [tmpSize], 'Cant Palabras Inicial': [tmpWordsOri],'Cant Palabras depuradas': [tmpWordsEnd],\"Vocabulario Inicial\":len(setWord),\"Vocabulario Final\":len(set_words)})\n",
    "        resultado = pd.concat([resultado, auxRes])\n",
    "        #break\n",
    "    else:\n",
    "        print(aux + ':' + str(f))\n",
    "\n",
    "resultado.to_csv(fileSummary, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Tamaño(K)</th>\n",
       "      <th>Cant Palabras Inicial</th>\n",
       "      <th>Cant Palabras depuradas</th>\n",
       "      <th>Vocabulario Inicial</th>\n",
       "      <th>Vocabulario Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1410.2670.txt</td>\n",
       "      <td>24</td>\n",
       "      <td>3926</td>\n",
       "      <td>3348</td>\n",
       "      <td>923</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1404.3626.txt</td>\n",
       "      <td>35</td>\n",
       "      <td>6247</td>\n",
       "      <td>4796</td>\n",
       "      <td>1052</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1405.0149.txt</td>\n",
       "      <td>34</td>\n",
       "      <td>7257</td>\n",
       "      <td>5240</td>\n",
       "      <td>907</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1409.2612.txt</td>\n",
       "      <td>21</td>\n",
       "      <td>4211</td>\n",
       "      <td>2960</td>\n",
       "      <td>506</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1511.00867.txt</td>\n",
       "      <td>72</td>\n",
       "      <td>14471</td>\n",
       "      <td>12117</td>\n",
       "      <td>1418</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Archivo  Tamaño(K)  Cant Palabras Inicial  Cant Palabras depuradas  \\\n",
       "0   1410.2670.txt         24                   3926                     3348   \n",
       "0   1404.3626.txt         35                   6247                     4796   \n",
       "0   1405.0149.txt         34                   7257                     5240   \n",
       "0   1409.2612.txt         21                   4211                     2960   \n",
       "0  1511.00867.txt         72                  14471                    12117   \n",
       "\n",
       "   Vocabulario Inicial  Vocabulario Final  \n",
       "0                  923                597  \n",
       "0                 1052                635  \n",
       "0                  907                580  \n",
       "0                  506                305  \n",
       "0                 1418                862  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70148"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import vstack,save_npz,load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construido el vocabulario podemos construir el bag of words, que se hace con la ayuda de la funcion CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",vocabulary =vocabulary , tokenizer = None, preprocessor = None, stop_words = 'english', max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70148"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('sparse_matrix.npz', train_data_features)\n",
    "#sparse_matrix = load_npz('sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0,\n",
       " 'aaa': 1,\n",
       " 'aaab': 2,\n",
       " 'aaacahichvhlssnafd': 3,\n",
       " 'aaacanichvhlssnafd': 4,\n",
       " 'aaacfxichvhlsinbfd': 5,\n",
       " 'aaacpnichvhlbhmxfd': 6,\n",
       " 'aaaczhichvhlssnafd': 7,\n",
       " 'aaai': 8,\n",
       " 'aaaibarri': 9,\n",
       " 'aab': 10,\n",
       " 'aaba': 11,\n",
       " 'aabaabc': 12,\n",
       " 'aabab': 13,\n",
       " 'aababa': 14,\n",
       " 'aababaababb': 15,\n",
       " 'aabb': 16,\n",
       " 'aabbb': 17,\n",
       " 'aabc': 18,\n",
       " 'aabcdaccaac': 19,\n",
       " 'aachen': 20,\n",
       " 'aad': 21,\n",
       " 'aaecc': 22,\n",
       " 'aaem': 23,\n",
       " 'aag': 24,\n",
       " 'aagaard': 25,\n",
       " 'aagea': 26,\n",
       " 'aai': 27,\n",
       " 'aaihaa': 28,\n",
       " 'aain': 29,\n",
       " 'aaini': 30,\n",
       " 'aaj': 31,\n",
       " 'aakv': 32,\n",
       " 'aalbersberg': 33,\n",
       " 'aalborg': 34,\n",
       " 'aalg': 35,\n",
       " 'aall': 36,\n",
       " 'aalto': 37,\n",
       " 'aam': 38,\n",
       " 'aama': 39,\n",
       " 'aamodt': 40,\n",
       " 'aan': 41,\n",
       " 'aana': 42,\n",
       " 'aand': 43,\n",
       " 'aanderaa': 44,\n",
       " 'aanstad': 45,\n",
       " 'aanund': 46,\n",
       " 'aarabi': 47,\n",
       " 'aardal': 48,\n",
       " 'aarhu': 49,\n",
       " 'aaron': 50,\n",
       " 'aaronson': 51,\n",
       " 'aart': 52,\n",
       " 'aarti': 53,\n",
       " 'aas': 54,\n",
       " 'aashtiani': 55,\n",
       " 'aat': 56,\n",
       " 'aau': 57,\n",
       " 'aaw': 58,\n",
       " 'aawcondit': 59,\n",
       " 'aazhang': 60,\n",
       " 'ab': 61,\n",
       " 'aba': 62,\n",
       " 'abab': 63,\n",
       " 'ababa': 64,\n",
       " 'ababac': 65,\n",
       " 'ababb': 66,\n",
       " 'ababc': 67,\n",
       " 'abac': 68,\n",
       " 'abacaxi': 69,\n",
       " 'abad': 70,\n",
       " 'abadi': 71,\n",
       " 'abaioff': 72,\n",
       " 'abaixo': 73,\n",
       " 'abak': 74,\n",
       " 'abalon': 75,\n",
       " 'abandon': 76,\n",
       " 'abaqu': 77,\n",
       " 'abarbanel': 78,\n",
       " 'abas': 79,\n",
       " 'abat': 80,\n",
       " 'abatzogl': 81,\n",
       " 'abb': 82,\n",
       " 'abba': 83,\n",
       " 'abbaa': 84,\n",
       " 'abbaac': 85,\n",
       " 'abbadi': 86,\n",
       " 'abbasi': 87,\n",
       " 'abbasiyadkori': 88,\n",
       " 'abbass': 89,\n",
       " 'abbbi': 90,\n",
       " 'abbeel': 91,\n",
       " 'abbey': 92,\n",
       " 'abbildung': 93,\n",
       " 'abbo': 94,\n",
       " 'abbott': 95,\n",
       " 'abboud': 96,\n",
       " 'abbrevi': 97,\n",
       " 'abc': 98,\n",
       " 'abca': 99,\n",
       " 'abcabbcabc': 100,\n",
       " 'abcba': 101,\n",
       " 'abcd': 102,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 103,\n",
       " 'abcp': 104,\n",
       " 'abcw': 105,\n",
       " 'abd': 106,\n",
       " 'abdallah': 107,\n",
       " 'abdedda': 108,\n",
       " 'abdeg': 109,\n",
       " 'abdel': 110,\n",
       " 'abdelberi': 111,\n",
       " 'abdelgawad': 112,\n",
       " 'abdelhak': 113,\n",
       " 'abdelhakim': 114,\n",
       " 'abdelhaq': 115,\n",
       " 'abdelkad': 116,\n",
       " 'abdellatif': 117,\n",
       " 'abdelouahab': 118,\n",
       " 'abdelrahman': 119,\n",
       " 'abdelzah': 120,\n",
       " 'abderrahim': 121,\n",
       " 'abdesselam': 122,\n",
       " 'abdolali': 123,\n",
       " 'abdoulay': 124,\n",
       " 'abduc': 125,\n",
       " 'abduct': 126,\n",
       " 'abdul': 127,\n",
       " 'abdula': 128,\n",
       " 'abdulkadirog': 129,\n",
       " 'abdulla': 130,\n",
       " 'abdullah': 131,\n",
       " 'abdulrahman': 132,\n",
       " 'abdur': 133,\n",
       " 'abe': 134,\n",
       " 'abeh': 135,\n",
       " 'abel': 136,\n",
       " 'abela': 137,\n",
       " 'abelian': 138,\n",
       " 'abello': 139,\n",
       " 'abelson': 140,\n",
       " 'abep': 141,\n",
       " 'aber': 142,\n",
       " 'abercrombi': 143,\n",
       " 'aberg': 144,\n",
       " 'abernethi': 145,\n",
       " 'aberth': 146,\n",
       " 'aberystwyth': 147,\n",
       " 'abeyesingh': 148,\n",
       " 'abgrenzung': 149,\n",
       " 'abgrp': 150,\n",
       " 'abh': 151,\n",
       " 'abhandlungen': 152,\n",
       " 'abhay': 153,\n",
       " 'abhaya': 154,\n",
       " 'abhi': 155,\n",
       " 'abhijeet': 156,\n",
       " 'abhijit': 157,\n",
       " 'abhik': 158,\n",
       " 'abhinav': 159,\n",
       " 'abhiruk': 160,\n",
       " 'abhishek': 161,\n",
       " 'abhradeep': 162,\n",
       " 'abhyankar': 163,\n",
       " 'abi': 164,\n",
       " 'abid': 165,\n",
       " 'abigail': 166,\n",
       " 'abii': 167,\n",
       " 'abil': 168,\n",
       " 'abilti': 169,\n",
       " 'abingdon': 170,\n",
       " 'abishanka': 171,\n",
       " 'abiteboul': 172,\n",
       " 'abj': 173,\n",
       " 'abjad': 174,\n",
       " 'abk': 175,\n",
       " 'abka': 176,\n",
       " 'abl': 177,\n",
       " 'ablat': 178,\n",
       " 'ablayev': 179,\n",
       " 'abley': 180,\n",
       " 'abli': 181,\n",
       " 'abm': 182,\n",
       " 'abmash': 183,\n",
       " 'abn': 184,\n",
       " 'abnorm': 185,\n",
       " 'abolhasan': 186,\n",
       " 'abolish': 187,\n",
       " 'abor': 188,\n",
       " 'aboratori': 189,\n",
       " 'abordagem': 190,\n",
       " 'abort': 191,\n",
       " 'abortship': 192,\n",
       " 'abouelleil': 193,\n",
       " 'aboulnasr': 194,\n",
       " 'abound': 195,\n",
       " 'aboutorab': 196,\n",
       " 'aboutp': 197,\n",
       " 'aboutt': 198,\n",
       " 'abovement': 199,\n",
       " 'abovepdefinit': 200,\n",
       " 'abovepmean': 201,\n",
       " 'abowd': 202,\n",
       " 'abox': 203,\n",
       " 'abp': 204,\n",
       " 'abpd': 205,\n",
       " 'abpl': 206,\n",
       " 'abpq': 207,\n",
       " 'abq': 208,\n",
       " 'abqp': 209,\n",
       " 'abr': 210,\n",
       " 'abraham': 211,\n",
       " 'abrahamson': 212,\n",
       " 'abram': 213,\n",
       " 'abramovich': 214,\n",
       " 'abramowitz': 215,\n",
       " 'abramski': 216,\n",
       " 'abramson': 217,\n",
       " 'abrego': 218,\n",
       " 'abreu': 219,\n",
       " 'abrial': 220,\n",
       " 'abridg': 221,\n",
       " 'abril': 222,\n",
       " 'abroad': 223,\n",
       " 'abrupt': 224,\n",
       " 'abruptli': 225,\n",
       " 'abrusci': 226,\n",
       " 'abrutyn': 227,\n",
       " 'abruzzi': 228,\n",
       " 'absb': 229,\n",
       " 'abscha': 230,\n",
       " 'abscissa': 231,\n",
       " 'abscoeff': 232,\n",
       " 'absenc': 233,\n",
       " 'absent': 234,\n",
       " 'absi': 235,\n",
       " 'absil': 236,\n",
       " 'absolut': 237,\n",
       " 'absolutecw': 238,\n",
       " 'absoluteleximin': 239,\n",
       " 'absorb': 240,\n",
       " 'absorpt': 241,\n",
       " 'absp': 242,\n",
       " 'abstent': 243,\n",
       " 'abstract': 244,\n",
       " 'abstractli': 245,\n",
       " 'abstractlinkedlist': 246,\n",
       " 'abstractorderedmapdecor': 247,\n",
       " 'abstractst': 248,\n",
       " 'abstrus': 249,\n",
       " 'absurd': 250,\n",
       " 'absurdo': 251,\n",
       " 'absurdum': 252,\n",
       " 'abt': 253,\n",
       " 'abtahi': 254,\n",
       " 'abu': 255,\n",
       " 'abualrub': 256,\n",
       " 'abubakr': 257,\n",
       " 'abugida': 258,\n",
       " 'abujarad': 259,\n",
       " 'abund': 260,\n",
       " 'abundanceecosystem': 261,\n",
       " 'abundanceproject': 262,\n",
       " 'abundantli': 263,\n",
       " 'abuot': 264,\n",
       " 'abur': 265,\n",
       " 'abus': 266,\n",
       " 'abut': 267,\n",
       " 'abuz': 268,\n",
       " 'abuzaid': 269,\n",
       " 'abv': 270,\n",
       " 'abw': 271,\n",
       " 'abx': 272,\n",
       " 'abya': 273,\n",
       " 'abz': 274,\n",
       " 'ac': 275,\n",
       " 'aca': 276,\n",
       " 'acad': 277,\n",
       " 'academ': 278,\n",
       " 'academi': 279,\n",
       " 'academia': 280,\n",
       " 'academiai': 281,\n",
       " 'academician': 282,\n",
       " 'acampora': 283,\n",
       " 'acapulco': 284,\n",
       " 'acar': 285,\n",
       " 'acarchau': 286,\n",
       " 'acaus': 287,\n",
       " 'acb': 288,\n",
       " 'acbc': 289,\n",
       " 'acbct': 290,\n",
       " 'acbd': 291,\n",
       " 'acbt': 292,\n",
       " 'acc': 293,\n",
       " 'accademia': 294,\n",
       " 'accardi': 295,\n",
       " 'accd': 296,\n",
       " 'accdhyp': 297,\n",
       " 'accel': 298,\n",
       " 'acceler': 299,\n",
       " 'acceleromet': 300,\n",
       " 'accent': 301,\n",
       " 'accentu': 302,\n",
       " 'accentur': 303,\n",
       " 'accept': 304,\n",
       " 'acceptor': 305,\n",
       " 'acces': 306,\n",
       " 'access': 307,\n",
       " 'accessiblilti': 308,\n",
       " 'accessor': 309,\n",
       " 'accessori': 310,\n",
       " 'accg': 311,\n",
       " 'accghyp': 312,\n",
       " 'acchyp': 313,\n",
       " 'acci': 314,\n",
       " 'accid': 315,\n",
       " 'accident': 316,\n",
       " 'accl': 317,\n",
       " 'acclaim': 318,\n",
       " 'accn': 319,\n",
       " 'acco': 320,\n",
       " 'accomazzi': 321,\n",
       " 'accommod': 322,\n",
       " 'accomod': 323,\n",
       " 'accompani': 324,\n",
       " 'accomplic': 325,\n",
       " 'accomplish': 326,\n",
       " 'accor': 327,\n",
       " 'accord': 328,\n",
       " 'accordi': 329,\n",
       " 'accordingli': 330,\n",
       " 'account': 331,\n",
       " 'accountmanag': 332,\n",
       " 'accountmanagerservic': 333,\n",
       " 'accoust': 334,\n",
       " 'accratef': 335,\n",
       " 'accratefhyp': 336,\n",
       " 'accrateg': 337,\n",
       " 'accrateghyp': 338,\n",
       " 'accratem': 339,\n",
       " 'accrel': 340,\n",
       " 'accru': 341,\n",
       " 'acct': 342,\n",
       " 'accuari': 343,\n",
       " 'accumul': 344,\n",
       " 'accur': 345,\n",
       " 'accuraci': 346,\n",
       " 'accus': 347,\n",
       " 'accv': 348,\n",
       " 'accx': 349,\n",
       " 'acd': 350,\n",
       " 'acdb': 351,\n",
       " 'ace': 352,\n",
       " 'acebro': 353,\n",
       " 'acebron': 354,\n",
       " 'acecomplet': 355,\n",
       " 'acemog': 356,\n",
       " 'acemoglu': 357,\n",
       " 'acentr': 358,\n",
       " 'acerbi': 359,\n",
       " 'acerca': 360,\n",
       " 'acero': 361,\n",
       " 'aceska': 362,\n",
       " 'aceto': 363,\n",
       " 'acevedo': 364,\n",
       " 'acf': 365,\n",
       " 'acfor': 366,\n",
       " 'acg': 367,\n",
       " 'acgt': 368,\n",
       " 'ach': 369,\n",
       " 'achan': 370,\n",
       " 'achanta': 371,\n",
       " 'achar': 372,\n",
       " 'achara': 373,\n",
       " 'achari': 374,\n",
       " 'acharya': 375,\n",
       " 'acheiev': 376,\n",
       " 'achi': 377,\n",
       " 'achieiv': 378,\n",
       " 'achiev': 379,\n",
       " 'achievabil': 380,\n",
       " 'achievabiliy': 381,\n",
       " 'achievabl': 382,\n",
       " 'achievebl': 383,\n",
       " 'achievedpsfrag': 384,\n",
       " 'achil': 385,\n",
       " 'achillea': 386,\n",
       " 'achilleo': 387,\n",
       " 'achim': 388,\n",
       " 'achin': 389,\n",
       " 'achiv': 390,\n",
       " 'achliopta': 391,\n",
       " 'achrekar': 392,\n",
       " 'achterberg': 393,\n",
       " 'achtergracht': 394,\n",
       " 'aci': 395,\n",
       " 'acial': 396,\n",
       " 'acid': 397,\n",
       " 'acifar': 398,\n",
       " 'acin': 399,\n",
       " 'acit': 400,\n",
       " 'aciv': 401,\n",
       " 'ack': 402,\n",
       " 'ackerman': 403,\n",
       " 'ackermann': 404,\n",
       " 'acket': 405,\n",
       " 'acki': 406,\n",
       " 'ackley': 407,\n",
       " 'acklm': 408,\n",
       " 'acknowledg': 409,\n",
       " 'acknowlegd': 410,\n",
       " 'ackoff': 411,\n",
       " 'ackowledg': 412,\n",
       " 'ackstrom': 413,\n",
       " 'acl': 414,\n",
       " 'aclf': 415,\n",
       " 'aclust': 416,\n",
       " 'acm': 417,\n",
       " 'acml': 418,\n",
       " 'acmmm': 419,\n",
       " 'acmsiam': 420,\n",
       " 'acn': 421,\n",
       " 'aco': 422,\n",
       " 'acolor': 423,\n",
       " 'acompi': 424,\n",
       " 'acomprehens': 425,\n",
       " 'aconsist': 426,\n",
       " 'aconst': 427,\n",
       " 'acontrol': 428,\n",
       " 'acopf': 429,\n",
       " 'acori': 430,\n",
       " 'acou': 431,\n",
       " 'acount': 432,\n",
       " 'acoust': 433,\n",
       " 'acp': 434,\n",
       " 'acpa': 435,\n",
       " 'acpj': 436,\n",
       " 'acqpa': 437,\n",
       " 'acquaint': 438,\n",
       " 'acquir': 439,\n",
       " 'acquisit': 440,\n",
       " 'acquisti': 441,\n",
       " 'acr': 442,\n",
       " 'acrf': 443,\n",
       " 'acri': 444,\n",
       " 'acro': 445,\n",
       " 'acrocel': 446,\n",
       " 'acronym': 447,\n",
       " 'across': 448,\n",
       " 'acsac': 449,\n",
       " 'acsd': 450,\n",
       " 'acsm': 451,\n",
       " 'acssc': 452,\n",
       " 'acsysimp': 453,\n",
       " 'act': 454,\n",
       " 'acta': 455,\n",
       " 'acteris': 456,\n",
       " 'acti': 457,\n",
       " 'actin': 458,\n",
       " 'action': 459,\n",
       " 'actionlabel': 460,\n",
       " 'actionrecord': 461,\n",
       " 'activ': 462,\n",
       " 'activationbas': 463,\n",
       " 'activationdesc': 464,\n",
       " 'activationgroup': 465,\n",
       " 'activationsystem': 466,\n",
       " 'activebodi': 467,\n",
       " 'activeset': 468,\n",
       " 'activist': 469,\n",
       " 'activit': 470,\n",
       " 'activityaspect': 471,\n",
       " 'activityedg': 472,\n",
       " 'activityedgeaspect': 473,\n",
       " 'activityedgeimpl': 474,\n",
       " 'activityimpl': 475,\n",
       " 'activitynet': 476,\n",
       " 'activitynod': 477,\n",
       " 'activitynodeactivationgroup': 478,\n",
       " 'activitynodeaspect': 479,\n",
       " 'activitynodeimpl': 480,\n",
       " 'activityparameternod': 481,\n",
       " 'activityu': 482,\n",
       " 'activityx': 483,\n",
       " 'acton': 484,\n",
       " 'actonto': 485,\n",
       " 'actor': 486,\n",
       " 'actorfoundri': 487,\n",
       " 'actorref': 488,\n",
       " 'actpbq': 489,\n",
       " 'actpq': 490,\n",
       " 'actpqq': 491,\n",
       " 'actq': 492,\n",
       " 'actscr': 493,\n",
       " 'actsh': 494,\n",
       " 'actshr': 495,\n",
       " 'actsi': 496,\n",
       " 'actslo': 497,\n",
       " 'actt': 498,\n",
       " 'actu': 499,\n",
       " 'actual': 500,\n",
       " 'actualis': 501,\n",
       " 'actualment': 502,\n",
       " 'actuari': 503,\n",
       " 'actuat': 504,\n",
       " 'acu': 505,\n",
       " 'acustica': 506,\n",
       " 'acut': 507,\n",
       " 'acvt': 508,\n",
       " 'acycl': 509,\n",
       " 'acyclicli': 510,\n",
       " 'acyl': 511,\n",
       " 'acz': 512,\n",
       " 'aczel': 513,\n",
       " 'ad': 514,\n",
       " 'ada': 515,\n",
       " 'adaboost': 516,\n",
       " 'adachi': 517,\n",
       " 'adadelta': 518,\n",
       " 'adadi': 519,\n",
       " 'adag': 520,\n",
       " 'adalbert': 521,\n",
       " 'adalf': 522,\n",
       " 'adalsteinsson': 523,\n",
       " 'adam': 524,\n",
       " 'adamatzki': 525,\n",
       " 'adamczak': 526,\n",
       " 'adamek': 527,\n",
       " 'adami': 528,\n",
       " 'adamsbashforth': 529,\n",
       " 'adamson': 530,\n",
       " 'adao': 531,\n",
       " 'adap': 532,\n",
       " 'adapt': 533,\n",
       " 'adapta': 534,\n",
       " 'adaptationcorrect': 535,\n",
       " 'adaptiverobust': 536,\n",
       " 'adar': 537,\n",
       " 'adaricheva': 538,\n",
       " 'adat': 539,\n",
       " 'adavani': 540,\n",
       " 'adc': 541,\n",
       " 'adcock': 542,\n",
       " 'adcol': 543,\n",
       " 'add': 544,\n",
       " 'addabbo': 545,\n",
       " 'addad': 546,\n",
       " 'addaiu': 547,\n",
       " 'addario': 548,\n",
       " 'addcandrul': 549,\n",
       " 'addclocklisten': 550,\n",
       " 'adddit': 551,\n",
       " 'addend': 552,\n",
       " 'addendum': 553,\n",
       " 'adder': 554,\n",
       " 'addflip': 555,\n",
       " 'addi': 556,\n",
       " 'addict': 557,\n",
       " 'addis': 558,\n",
       " 'addison': 559,\n",
       " 'addisonwesley': 560,\n",
       " 'addisson': 561,\n",
       " 'addit': 562,\n",
       " 'additio': 563,\n",
       " 'additiveacross': 564,\n",
       " 'additiveerror': 565,\n",
       " 'additiven': 566,\n",
       " 'addiu': 567,\n",
       " 'addnod': 568,\n",
       " 'addon': 569,\n",
       " 'addr': 570,\n",
       " 'addrandomappl': 571,\n",
       " 'address': 572,\n",
       " 'addresse': 573,\n",
       " 'addscor': 574,\n",
       " 'addscorei': 575,\n",
       " 'addtion': 576,\n",
       " 'addu': 577,\n",
       " 'adduc': 578,\n",
       " 'ade': 579,\n",
       " 'adecod': 580,\n",
       " 'adel': 581,\n",
       " 'adelaid': 582,\n",
       " 'adelgren': 583,\n",
       " 'adelin': 584,\n",
       " 'adelman': 585,\n",
       " 'adelmann': 586,\n",
       " 'adelson': 587,\n",
       " 'aden': 588,\n",
       " 'adenin': 589,\n",
       " 'adequ': 590,\n",
       " 'adequaci': 591,\n",
       " 'ader': 592,\n",
       " 'aderiv': 593,\n",
       " 'adesnik': 594,\n",
       " 'adesso': 595,\n",
       " 'adetunji': 596,\n",
       " 'adewol': 597,\n",
       " 'adford': 598,\n",
       " 'adfsdca': 599,\n",
       " 'adg': 600,\n",
       " 'adga': 601,\n",
       " 'adham': 602,\n",
       " 'adher': 603,\n",
       " 'adhes': 604,\n",
       " 'adhikari': 605,\n",
       " 'adhoc': 606,\n",
       " 'adhu': 607,\n",
       " 'adi': 608,\n",
       " 'adiabat': 609,\n",
       " 'adiac': 610,\n",
       " 'adibi': 611,\n",
       " 'adic': 612,\n",
       " 'adida': 613,\n",
       " 'adifor': 614,\n",
       " 'adigit': 615,\n",
       " 'adijac': 616,\n",
       " 'adilson': 617,\n",
       " 'adimat': 618,\n",
       " 'adio': 619,\n",
       " 'adipos': 620,\n",
       " 'adiqu': 621,\n",
       " 'adish': 622,\n",
       " 'adistm': 623,\n",
       " 'adit': 624,\n",
       " 'aditya': 625,\n",
       " 'adiwena': 626,\n",
       " 'adj': 627,\n",
       " 'adjac': 628,\n",
       " 'adjacent': 629,\n",
       " 'adjacentclos': 630,\n",
       " 'adjacentclosur': 631,\n",
       " 'adject': 632,\n",
       " 'adji': 633,\n",
       " 'adjnoun': 634,\n",
       " 'adjoin': 635,\n",
       " 'adjoint': 636,\n",
       " 'adjud': 637,\n",
       " 'adjug': 638,\n",
       " 'adjunct': 639,\n",
       " 'adjust': 640,\n",
       " 'adjustp': 641,\n",
       " 'adk': 642,\n",
       " 'adl': 643,\n",
       " 'adlakha': 644,\n",
       " 'adleman': 645,\n",
       " 'adler': 646,\n",
       " 'adma': 647,\n",
       " 'admetox': 648,\n",
       " 'admin': 649,\n",
       " 'administ': 650,\n",
       " 'administr': 651,\n",
       " 'admir': 652,\n",
       " 'admira': 653,\n",
       " 'admiss': 654,\n",
       " 'admistr': 655,\n",
       " 'admit': 656,\n",
       " 'admitt': 657,\n",
       " 'admittedli': 658,\n",
       " 'admm': 659,\n",
       " 'adn': 660,\n",
       " 'adnhztyclxk': 661,\n",
       " 'adnot': 662,\n",
       " 'ado': 663,\n",
       " 'adob': 664,\n",
       " 'adol': 665,\n",
       " 'adolesc': 666,\n",
       " 'adolfo': 667,\n",
       " 'adom': 668,\n",
       " 'adomaviciu': 669,\n",
       " 'adopt': 670,\n",
       " 'adorn': 671,\n",
       " 'adp': 672,\n",
       " 'adpk': 673,\n",
       " 'adpoint': 674,\n",
       " 'adpt': 675,\n",
       " 'adptiv': 676,\n",
       " 'adq': 677,\n",
       " 'adr': 678,\n",
       " 'adress': 679,\n",
       " 'adresses': 680,\n",
       " 'adria': 681,\n",
       " 'adrian': 682,\n",
       " 'adriana': 683,\n",
       " 'adriano': 684,\n",
       " 'adrien': 685,\n",
       " 'adsaf': 686,\n",
       " 'adsolv': 687,\n",
       " 'adsorpt': 688,\n",
       " 'adt': 689,\n",
       " 'adtk': 690,\n",
       " 'adu': 691,\n",
       " 'adult': 692,\n",
       " 'adulthood': 693,\n",
       " 'adusumilli': 694,\n",
       " 'adv': 695,\n",
       " 'advanc': 696,\n",
       " 'advancepoint': 697,\n",
       " 'advancingfront': 698,\n",
       " 'advantag': 699,\n",
       " 'advcdh': 700,\n",
       " 'advclass': 701,\n",
       " 'advdummi': 702,\n",
       " 'advect': 703,\n",
       " 'advectivediffus': 704,\n",
       " 'advent': 705,\n",
       " 'adventur': 706,\n",
       " 'advers': 707,\n",
       " 'adversari': 708,\n",
       " 'advertis': 709,\n",
       " 'advic': 710,\n",
       " 'advis': 711,\n",
       " 'advisor': 712,\n",
       " 'advisori': 713,\n",
       " 'advoc': 714,\n",
       " 'advocaci': 715,\n",
       " 'advol': 716,\n",
       " 'adwait': 717,\n",
       " 'adword': 718,\n",
       " 'adx': 719,\n",
       " 'adz': 720,\n",
       " 'ae': 721,\n",
       " 'aeb': 722,\n",
       " 'aecor': 723,\n",
       " 'aee': 724,\n",
       " 'aef': 725,\n",
       " 'aegean': 726,\n",
       " 'aegi': 727,\n",
       " 'aei': 728,\n",
       " 'aej': 729,\n",
       " 'aejb': 730,\n",
       " 'aek': 731,\n",
       " 'aekd': 732,\n",
       " 'ael': 733,\n",
       " 'aella': 734,\n",
       " 'aem': 735,\n",
       " 'aen': 736,\n",
       " 'aenc': 737,\n",
       " 'aeolian': 738,\n",
       " 'aeolu': 739,\n",
       " 'aep': 740,\n",
       " 'aeq': 741,\n",
       " 'aequation': 742,\n",
       " 'aeqxagfbnlulzhpzmpynip': 743,\n",
       " 'aerial': 744,\n",
       " 'aero': 745,\n",
       " 'aeroacoust': 746,\n",
       " 'aerodynam': 747,\n",
       " 'aerofoil': 748,\n",
       " 'aeron': 749,\n",
       " 'aeronaut': 750,\n",
       " 'aeroplan': 751,\n",
       " 'aerosp': 752,\n",
       " 'aerospac': 753,\n",
       " 'aert': 754,\n",
       " 'aesn': 755,\n",
       " 'aesop': 756,\n",
       " 'aesthet': 757,\n",
       " 'aeterna': 758,\n",
       " 'aexp': 759,\n",
       " 'aexppol': 760,\n",
       " 'aext': 761,\n",
       " 'aeyel': 762,\n",
       " 'af': 763,\n",
       " 'afa': 764,\n",
       " 'afactor': 765,\n",
       " 'afanasiev': 766,\n",
       " 'afanasyev': 767,\n",
       " 'afar': 768,\n",
       " 'afc': 769,\n",
       " 'afcet': 770,\n",
       " 'afcetsmf': 771,\n",
       " 'afek': 772,\n",
       " 'afem': 773,\n",
       " 'aff': 774,\n",
       " 'affair': 775,\n",
       " 'affdim': 776,\n",
       " 'affect': 777,\n",
       " 'affection': 778,\n",
       " 'affer': 779,\n",
       " 'affil': 780,\n",
       " 'affili': 781,\n",
       " 'affin': 782,\n",
       " 'affirm': 783,\n",
       " 'affix': 784,\n",
       " 'afflict': 785,\n",
       " 'affonso': 786,\n",
       " 'afford': 787,\n",
       " 'affymetrix': 788,\n",
       " 'afh': 789,\n",
       " 'afi': 790,\n",
       " 'afield': 791,\n",
       " 'afip': 792,\n",
       " 'afivo': 793,\n",
       " 'afk': 794,\n",
       " 'afl': 795,\n",
       " 'aflalo': 796,\n",
       " 'aflw': 797,\n",
       " 'afm': 798,\n",
       " 'afma': 799,\n",
       " 'afmm': 800,\n",
       " 'afn': 801,\n",
       " 'afonso': 802,\n",
       " 'afor': 803,\n",
       " 'aforement': 804,\n",
       " 'aforesaid': 805,\n",
       " 'afosr': 806,\n",
       " 'afosrfa': 807,\n",
       " 'afoul': 808,\n",
       " 'afp': 809,\n",
       " 'afra': 810,\n",
       " 'afrabandpey': 811,\n",
       " 'afrabandpeyb': 812,\n",
       " 'afraid': 813,\n",
       " 'afrati': 814,\n",
       " 'afresh': 815,\n",
       " 'afriat': 816,\n",
       " 'africa': 817,\n",
       " 'african': 818,\n",
       " 'africon': 819,\n",
       " 'afrl': 820,\n",
       " 'afro': 821,\n",
       " 'afschem': 822,\n",
       " 'afshang': 823,\n",
       " 'afshar': 824,\n",
       " 'afshin': 825,\n",
       " 'aft': 826,\n",
       " 'aftan': 827,\n",
       " 'afteract': 828,\n",
       " 'aftermath': 829,\n",
       " 'afternoon': 830,\n",
       " 'aftershock': 831,\n",
       " 'aftersremov': 832,\n",
       " 'afterward': 833,\n",
       " 'afterwardsin': 834,\n",
       " 'afterword': 835,\n",
       " 'aftn': 836,\n",
       " 'afu': 837,\n",
       " 'afuk': 838,\n",
       " 'afw': 839,\n",
       " 'afz': 840,\n",
       " 'ag': 841,\n",
       " 'aga': 842,\n",
       " 'agadzanyan': 843,\n",
       " 'agafonov': 844,\n",
       " 'agaian': 845,\n",
       " 'agapi': 846,\n",
       " 'agapito': 847,\n",
       " 'agaraj': 848,\n",
       " 'agaraja': 849,\n",
       " 'agarawala': 850,\n",
       " 'agard': 851,\n",
       " 'agarw': 852,\n",
       " 'agarwala': 853,\n",
       " 'agatho': 854,\n",
       " 'agatz': 855,\n",
       " 'agazin': 856,\n",
       " 'agc': 857,\n",
       " 'agct': 858,\n",
       " 'agcta': 859,\n",
       " 'agda': 860,\n",
       " 'age': 861,\n",
       " 'ageev': 862,\n",
       " 'ageman': 863,\n",
       " 'agenc': 864,\n",
       " 'agenda': 865,\n",
       " 'agent': 866,\n",
       " 'agentbas': 867,\n",
       " 'agentmodel': 868,\n",
       " 'agentp': 869,\n",
       " 'agentservicetheori': 870,\n",
       " 'agentspecif': 871,\n",
       " 'ager': 872,\n",
       " 'agesh': 873,\n",
       " 'aget': 874,\n",
       " 'agevari': 875,\n",
       " 'agfa': 876,\n",
       " 'agg': 877,\n",
       " 'aggarw': 878,\n",
       " 'aggelo': 879,\n",
       " 'agglom': 880,\n",
       " 'agglomer': 881,\n",
       " 'agglomera': 882,\n",
       " 'aggoun': 883,\n",
       " 'aggrav': 884,\n",
       " 'aggreg': 885,\n",
       " 'aggregat': 886,\n",
       " 'aggres': 887,\n",
       " 'aggress': 888,\n",
       " 'aggverifi': 889,\n",
       " 'agh': 890,\n",
       " 'agha': 891,\n",
       " 'aghabozorgi': 892,\n",
       " 'aghae': 893,\n",
       " 'aghannan': 894,\n",
       " 'aghaven': 895,\n",
       " 'aghu': 896,\n",
       " 'agi': 897,\n",
       " 'agichtein': 898,\n",
       " 'agil': 899,\n",
       " 'agiomyrgiannaki': 900,\n",
       " 'agit': 901,\n",
       " 'agiu': 902,\n",
       " 'agiv': 903,\n",
       " 'agl': 904,\n",
       " 'agliano': 905,\n",
       " 'aglidereiread': 906,\n",
       " 'agliderseiread': 907,\n",
       " 'agm': 908,\n",
       " 'agmon': 909,\n",
       " 'agn': 910,\n",
       " 'agnar': 911,\n",
       " 'agneessen': 912,\n",
       " 'agner': 913,\n",
       " 'agnieszka': 914,\n",
       " 'agnihotri': 915,\n",
       " 'agnost': 916,\n",
       " 'ago': 917,\n",
       " 'agol': 918,\n",
       " 'agon': 919,\n",
       " 'agonist': 920,\n",
       " 'agostino': 921,\n",
       " 'agotn': 922,\n",
       " 'agp': 923,\n",
       " 'agr': 924,\n",
       " 'agrachev': 925,\n",
       " 'agraw': 926,\n",
       " 'agre': 927,\n",
       " 'agreement': 928,\n",
       " 'agresti': 929,\n",
       " 'agribio': 930,\n",
       " 'agricultur': 931,\n",
       " 'agroparistech': 932,\n",
       " 'agrophi': 933,\n",
       " 'agt': 934,\n",
       " 'aguayo': 935,\n",
       " 'aguc': 936,\n",
       " 'agudelo': 937,\n",
       " 'agueda': 938,\n",
       " 'aguiar': 939,\n",
       " 'aguilar': 940,\n",
       " 'aguilera': 941,\n",
       " 'aguillo': 942,\n",
       " 'aguirr': 943,\n",
       " 'agullo': 944,\n",
       " 'agustin': 945,\n",
       " 'agustino': 946,\n",
       " 'agv': 947,\n",
       " 'agvm': 948,\n",
       " 'ah': 949,\n",
       " 'aha': 950,\n",
       " 'ahai': 951,\n",
       " 'ahajan': 952,\n",
       " 'aharia': 953,\n",
       " 'aharon': 954,\n",
       " 'aharoni': 955,\n",
       " 'aharonov': 956,\n",
       " 'ahasanun': 957,\n",
       " 'ahb': 958,\n",
       " 'ahc': 959,\n",
       " 'ahdki': 960,\n",
       " 'ahe': 961,\n",
       " 'ahead': 962,\n",
       " 'ahel': 963,\n",
       " 'ahern': 964,\n",
       " 'ahh': 965,\n",
       " 'ahhi': 966,\n",
       " 'ahi': 967,\n",
       " 'ahigh': 968,\n",
       " 'ahipasaoglu': 969,\n",
       " 'ahk': 970,\n",
       " 'ahl': 971,\n",
       " 'ahlgren': 972,\n",
       " 'ahlswed': 973,\n",
       " 'ahm': 974,\n",
       " 'ahmad': 975,\n",
       " 'ahmadabadi': 976,\n",
       " 'ahmadi': 977,\n",
       " 'ahmadia': 978,\n",
       " 'ahmadian': 979,\n",
       " 'ahmet': 980,\n",
       " 'ahn': 981,\n",
       " 'aho': 982,\n",
       " 'aholt': 983,\n",
       " 'ahonen': 984,\n",
       " 'ahoney': 985,\n",
       " 'ahp': 986,\n",
       " 'ahpi': 987,\n",
       " 'ahrc': 988,\n",
       " 'ahren': 989,\n",
       " 'ahrweil': 990,\n",
       " 'ahuja': 991,\n",
       " 'ahv': 992,\n",
       " 'ai': 993,\n",
       " 'aiaa': 994,\n",
       " 'aib': 995,\n",
       " 'aibo': 996,\n",
       " 'aibx': 997,\n",
       " 'aic': 998,\n",
       " 'aicc': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'BoW1.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))\n",
    "#loaded_model = pickle.load(open('BoW1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(vocabulary - words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
