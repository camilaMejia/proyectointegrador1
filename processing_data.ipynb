{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_file_path es la ruta donde se encuentran los archivos de texto plano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  Path(\"~\").expanduser().resolve()\n",
    "#base_path = Path.cwd().expanduser().resolve()\n",
    "input_file_path  = base_path / 'datasets/papers-txt/'\n",
    "#datasetOut =base_path / \"datasets/salidas_procesamiento/\"\n",
    "#datasetOut_freq = base_path / \"datasets/salidas_freq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cmejia3/datasets/papers-txt')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileSize(fileIn):\n",
    "    size=os.stat(fileIn).st_size\n",
    "    return size\n",
    "\n",
    "def wordsCount(contenido):\n",
    "    totWwords=contenido.split()\n",
    "    return len(totWwords)\n",
    "\n",
    "def cleanWordCount(contenido):\n",
    "    contenido =re.sub('((f|ht)tp(s?)://)?(.*)[.][a-z]+(((\\/.*(\\/?)){1,}?)(.*([.].*)?))?',' ',contenido) # Eliminar las URL\n",
    "    #contenido =re.sub('REFERENCES (\\S|\\w)+',' ',contenido) # Eliminar la bibliografia\n",
    "    contenido =re.sub('[a-zA-Z0-9.?{}]+@\\w+\\.\\w+.\\w*',' ',contenido) # Eliminar los correos\n",
    "    contenido =re.sub('\\[[a-zA-Z0-9\\,\\. ]+\\]',' ',contenido) # Eliminar cualquier contenido entre corchetes\n",
    "    contenido =re.sub('\\([a-zA-Z0-9\\,\\.\\- ]+\\)',' ',contenido) # Eliminar cualquier contenido entre paréntesis\n",
    "    contenido =re.sub('((et al\\.)|(i\\.i\\.d\\.)|(i\\.e\\.)|\\'|\\’|\\`)',' ',contenido) # Eliminar abreviaciones, apostrofes y guion\n",
    "    contenido =re.sub('(á|à|ä)','a',contenido) # Reemplazar a acentuada\n",
    "    contenido =re.sub('(é|è|ë)','e',contenido) # Reemplazar e acentuada\n",
    "    contenido =re.sub('(í|ì|ï)','i',contenido) # Reemplazar i acentuada\n",
    "    contenido =re.sub('(ó|ò|ö)','o',contenido) # Reemplazar o acentuada\n",
    "    contenido =re.sub('(ú|ù|ü)','u',contenido) # Reemplazar u acentuada\n",
    "    contenido =re.sub('-|\\u2212|\\u2012|\\u2013|\\u2014|\\u2015',' ',contenido) # Reemplazar el guión\n",
    "    contenido =re.sub('[^a-zA-Z]',' ',contenido) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    contenido =re.sub('(a-z|A-Z){1,2}',' ',contenido) # Eliminar palabras o números de un caracter de longitud \n",
    "    contenido =re.sub('((\\w*)?xyz(\\w*)?)',' ',contenido) # Eliminar palabras que tiene la cadena xyz \n",
    "    contenido =re.sub('((\\w*)?abc(\\w*)?)',' ',contenido) # Eliminar palabras que tiene la cadena abc\n",
    "    contenido =re.sub(' +',' ',contenido) # Eliminar espacios en blanco\n",
    "    contenidoDuplicado =re.findall(r'(\\w*?(\\w)\\2{2,}.*?)',contenido) #Elminar carecteres repetidos\n",
    "    i=0\n",
    "    longDuplicados=len(contenidoDuplicado)\n",
    "    while i <longDuplicados:\n",
    "        pattern= \"\\w*\" + contenidoDuplicado [i][0] +\"\\w*\"\n",
    "        contenido=re.sub(pattern,' ',contenido) # Eliminar las URL\n",
    "        i=i+1\n",
    "    #contenido =re.sub('[^A-Za-z0-9.,_%+-\\(\\)\\[\\]\\´\\'\\`]',' ',contenido)\n",
    "    #contenido =re.sub('\\[(0-9)+\\]',' ',contenido)    \n",
    "    #totWordDepurado = Counter(map(str, contenido.split()))\n",
    "    #outputFile= open(datasetOut, 'w', encoding='UTF-8')\n",
    "    #outputFile.write(contenido)\n",
    "    #outputFile.close()\n",
    "    totWwords=contenido.split()\n",
    "    setWords = set(totWwords)\n",
    "    #print(\"Total de palabras {}\".format(len(totWwords)))\n",
    "    #print(\"Total de palabras después del pre-procesamiento: {}\".format(totWordDepurado))\n",
    "    return len(totWwords),contenido,setWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_ratio(input):\n",
    "    lang_ratio = {}\n",
    "    tokens = wordpunct_tokenize(input)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        word_set = set(words)\n",
    "        common_elements = word_set.intersection(stopwords_set)\n",
    "        lang_ratio[language] = len(common_elements)\n",
    "    return lang_ratio\n",
    "\n",
    "def detect_language(input):\n",
    "    ratios = lang_ratio(input)\n",
    "    language = max(ratios, key = ratios.get)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de las stop words que serán eliminadas ya que no aportan valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras que considermos StopWords que no estan incluidas en el conjunto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoStopWords = ['www','https','html','figure', 'chapter','abbcbccabcabcabcabcbcbabacba','abcdefghijklmnopqrstuvwxyz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añaden las palabras que consideramos al conjunto principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.extend(listoStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función principal procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(texto,stopWords):\n",
    "    \n",
    "    #Quitar todos los acentos\n",
    "    #texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    #Quitar todos los caracteres especiales\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto)\n",
    "    \n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    #Variable que guarda el año en el que estamos que es el limite superior de los números que no se van a eliminar\n",
    "    currentYear = int(dt.datetime.now().year)\n",
    "    \n",
    "    #Verificar que las palabras tengan más de un caracter, que además sean solo sean letras\n",
    "    # o si son números que esten entre un rango que sea admisible para no eliminar información de año que se mencione en los artículos\n",
    "    # y finalmente que no sean palabras que estan en el dicccionario de stopwords.\n",
    "    \n",
    "    #tokens = [w for w in tokens if (len(w)>1)&(w.isalpha() or (w.isnumeric() and int(w)>=1800 and int(w)<=currentYear))&(w not in stopWords)]\n",
    "    tokens = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "    \n",
    "    matcher= re.compile(r'(.)\\1*')\n",
    "    #tokens1 = []\n",
    "    #for word in tokens:\n",
    "        #aux = [len(match.group()) for match in matcher.finditer(word)]\n",
    "        #if max(aux)<=3:\n",
    "            #tokens1.append(word)\n",
    "        #else:\n",
    "            #print(word)\n",
    "    \n",
    "    \n",
    "    #tokens1 = [w for w in tokens1 if (Counter(w).most_common(1)[0][1]<5)]\n",
    "    #eliminadas = [w for w in tokens1 if (Counter(w).most_common(1)[0][1]>=5)]\n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens = [ps.stem(w) for w in tokens]\n",
    "    \n",
    "    #Lematización\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "\n",
    "    tokens = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens]\n",
    "    \n",
    "    #Se retorna el texto nuevamente en un solo string luego de ser procesado\n",
    "    to_return = ' '.join(tokens)\n",
    "    \n",
    "    #Se retorna el vocabulario de cada documento\n",
    "    set_words = set(tokens)\n",
    "    \n",
    "    #Y la frecuencia de las palabras\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    return to_return,set_words,freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de los conjuntos:\n",
    "\n",
    "    - Vocabulary: el conjunto de todas las palabras que contienen los documentos\n",
    "    - results_text: la lista con los documentos ya organizados para construir el bag of words\n",
    "    - results_frecuency: información de cada documento de las palabras que contiene cuántas veces las contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "results_text = []\n",
    "results_frecuency = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german:/home/cmejia3/datasets/papers-txt/1508.02340.txt\n"
     ]
    }
   ],
   "source": [
    "fileSummary = \"CleanSummary.csv\"\n",
    "\n",
    "contenido= \"Archivo\" + \";\" + \"Tamaño(K)\" + \";\" + \"Cant Palabras Inicial\" + \";\" + \"Cant Palabras depuradas\"+ \";\" +\"Porc Limpieza\"+ \"\\n\"\n",
    "resultado = pd.DataFrame()\n",
    "indexFiles = []\n",
    "documents = []\n",
    "\n",
    "for f in input_file_path.glob('*.txt'):\n",
    "    #Peso del archivo\n",
    "    tmpSize=round(fileSize(f)/1014)\n",
    "    \n",
    "    #Lectura del archivo\n",
    "    input_file = open(f, \"r\", encoding = 'utf-8')\n",
    "    input_aux = input_file.read()\n",
    "    \n",
    "    #Cuenta de palabras iniciales\n",
    "    tmpWordsOri=wordsCount(input_aux)\n",
    "    #out = datasetOut / str(f).split('/')[-1]\n",
    "    \n",
    "    #Cuenta de palabras despues de la limpieza\n",
    "    tmpWordsEnd,text,setWord=cleanWordCount(input_aux)\n",
    "    #tmpPerClean=round((tmpWordsEnd/tmpWordsOri)*100)\n",
    "    \n",
    "    \n",
    "    #Detectando el idioma\n",
    "    aux = detect_language(text)\n",
    "    \n",
    "    if(aux == 'english'):\n",
    "        text_cleanned,set_words,freq = clean_files(text,stopWords)\n",
    "        #out2 = datasetOut / str(f).split('/')[-1]\n",
    "        documents.append(text_cleanned)\n",
    "        vocabulary = vocabulary.union(set_words)\n",
    "        results_text.append(text_cleanned)\n",
    "        indexFiles.append(str(f).split('/')[-1])\n",
    "        \n",
    "        #Escritura de los resultados del preprocesamiento\n",
    "        auxRes= pd.DataFrame({'Archivo': str(f).split('/')[-1], 'Tamaño(K)': [tmpSize], 'Cant Palabras Inicial': [tmpWordsOri],'Cant Palabras depuradas': [tmpWordsEnd],\"Vocabulario Inicial\":len(setWord),\"Vocabulario Final\":len(set_words)})\n",
    "        resultado = pd.concat([resultado, auxRes])\n",
    "        #break\n",
    "    else:\n",
    "        print(aux + ':' + str(f))\n",
    "\n",
    "resultado.to_csv(fileSummary, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Tamaño(K)</th>\n",
       "      <th>Cant Palabras Inicial</th>\n",
       "      <th>Cant Palabras depuradas</th>\n",
       "      <th>Vocabulario Inicial</th>\n",
       "      <th>Vocabulario Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1410.2670.txt</td>\n",
       "      <td>24</td>\n",
       "      <td>3926</td>\n",
       "      <td>3357</td>\n",
       "      <td>924</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1404.3626.txt</td>\n",
       "      <td>35</td>\n",
       "      <td>6247</td>\n",
       "      <td>4866</td>\n",
       "      <td>1033</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1405.0149.txt</td>\n",
       "      <td>34</td>\n",
       "      <td>7257</td>\n",
       "      <td>5219</td>\n",
       "      <td>882</td>\n",
       "      <td>579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1409.2612.txt</td>\n",
       "      <td>21</td>\n",
       "      <td>4211</td>\n",
       "      <td>2953</td>\n",
       "      <td>504</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1511.00867.txt</td>\n",
       "      <td>72</td>\n",
       "      <td>14471</td>\n",
       "      <td>12283</td>\n",
       "      <td>1373</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Archivo  Tamaño(K)  Cant Palabras Inicial  Cant Palabras depuradas  \\\n",
       "0   1410.2670.txt         24                   3926                     3357   \n",
       "0   1404.3626.txt         35                   6247                     4866   \n",
       "0   1405.0149.txt         34                   7257                     5219   \n",
       "0   1409.2612.txt         21                   4211                     2953   \n",
       "0  1511.00867.txt         72                  14471                    12283   \n",
       "\n",
       "   Vocabulario Inicial  Vocabulario Final  \n",
       "0                  924                597  \n",
       "0                 1033                634  \n",
       "0                  882                579  \n",
       "0                  504                305  \n",
       "0                 1373                857  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69770"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import vstack,save_npz,load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construido el vocabulario podemos construir el bag of words, que se hace con la ayuda de la funcion CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",vocabulary =vocabulary , tokenizer = None, preprocessor = None, stop_words = 'english', max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69770"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('sparse_matrix.npz', train_data_features)\n",
    "#sparse_matrix = load_npz('sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 0,\n",
       " 'aaa': 1,\n",
       " 'aab': 2,\n",
       " 'aaba': 3,\n",
       " 'aabab': 4,\n",
       " 'aababa': 5,\n",
       " 'aababaababb': 6,\n",
       " 'aabb': 7,\n",
       " 'aachen': 8,\n",
       " 'aad': 9,\n",
       " 'aaecc': 10,\n",
       " 'aaem': 11,\n",
       " 'aag': 12,\n",
       " 'aagaard': 13,\n",
       " 'aagea': 14,\n",
       " 'aai': 15,\n",
       " 'aaihaa': 16,\n",
       " 'aain': 17,\n",
       " 'aaini': 18,\n",
       " 'aaj': 19,\n",
       " 'aakv': 20,\n",
       " 'aalbersberg': 21,\n",
       " 'aalborg': 22,\n",
       " 'aalg': 23,\n",
       " 'aall': 24,\n",
       " 'aalto': 25,\n",
       " 'aam': 26,\n",
       " 'aama': 27,\n",
       " 'aamodt': 28,\n",
       " 'aan': 29,\n",
       " 'aana': 30,\n",
       " 'aand': 31,\n",
       " 'aanderaa': 32,\n",
       " 'aanstad': 33,\n",
       " 'aanund': 34,\n",
       " 'aarabi': 35,\n",
       " 'aardal': 36,\n",
       " 'aarhu': 37,\n",
       " 'aaron': 38,\n",
       " 'aaronson': 39,\n",
       " 'aart': 40,\n",
       " 'aarti': 41,\n",
       " 'aas': 42,\n",
       " 'aashtiani': 43,\n",
       " 'aat': 44,\n",
       " 'aau': 45,\n",
       " 'aaw': 46,\n",
       " 'aawcondit': 47,\n",
       " 'aazhang': 48,\n",
       " 'ab': 49,\n",
       " 'aba': 50,\n",
       " 'abab': 51,\n",
       " 'ababa': 52,\n",
       " 'ababac': 53,\n",
       " 'ababb': 54,\n",
       " 'abac': 55,\n",
       " 'abacaxi': 56,\n",
       " 'abad': 57,\n",
       " 'abadi': 58,\n",
       " 'abaioff': 59,\n",
       " 'abaixo': 60,\n",
       " 'abak': 61,\n",
       " 'abalon': 62,\n",
       " 'abandon': 63,\n",
       " 'abaqu': 64,\n",
       " 'abarbanel': 65,\n",
       " 'abas': 66,\n",
       " 'abat': 67,\n",
       " 'abatzogl': 68,\n",
       " 'abb': 69,\n",
       " 'abba': 70,\n",
       " 'abbaa': 71,\n",
       " 'abbaac': 72,\n",
       " 'abbadi': 73,\n",
       " 'abbasi': 74,\n",
       " 'abbasiyadkori': 75,\n",
       " 'abbass': 76,\n",
       " 'abbeel': 77,\n",
       " 'abbey': 78,\n",
       " 'abbildung': 79,\n",
       " 'abbo': 80,\n",
       " 'abbott': 81,\n",
       " 'abboud': 82,\n",
       " 'abbrevi': 83,\n",
       " 'abc': 84,\n",
       " 'abcd': 85,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 86,\n",
       " 'abcp': 87,\n",
       " 'abd': 88,\n",
       " 'abdallah': 89,\n",
       " 'abdedda': 90,\n",
       " 'abdeg': 91,\n",
       " 'abdel': 92,\n",
       " 'abdelberi': 93,\n",
       " 'abdelgawad': 94,\n",
       " 'abdelhak': 95,\n",
       " 'abdelhakim': 96,\n",
       " 'abdelhaq': 97,\n",
       " 'abdelkad': 98,\n",
       " 'abdellatif': 99,\n",
       " 'abdelouahab': 100,\n",
       " 'abdelrahman': 101,\n",
       " 'abdelzah': 102,\n",
       " 'abderrahim': 103,\n",
       " 'abdesselam': 104,\n",
       " 'abdolali': 105,\n",
       " 'abdoulay': 106,\n",
       " 'abduc': 107,\n",
       " 'abduct': 108,\n",
       " 'abdul': 109,\n",
       " 'abdula': 110,\n",
       " 'abdulkadirog': 111,\n",
       " 'abdulla': 112,\n",
       " 'abdullah': 113,\n",
       " 'abdulrahman': 114,\n",
       " 'abdur': 115,\n",
       " 'abe': 116,\n",
       " 'abeh': 117,\n",
       " 'abel': 118,\n",
       " 'abela': 119,\n",
       " 'abelian': 120,\n",
       " 'abello': 121,\n",
       " 'abelson': 122,\n",
       " 'abep': 123,\n",
       " 'aber': 124,\n",
       " 'abercrombi': 125,\n",
       " 'aberg': 126,\n",
       " 'abernethi': 127,\n",
       " 'aberth': 128,\n",
       " 'aberystwyth': 129,\n",
       " 'abeyesingh': 130,\n",
       " 'abgrenzung': 131,\n",
       " 'abgrp': 132,\n",
       " 'abh': 133,\n",
       " 'abhandlungen': 134,\n",
       " 'abhay': 135,\n",
       " 'abhaya': 136,\n",
       " 'abhi': 137,\n",
       " 'abhijeet': 138,\n",
       " 'abhijit': 139,\n",
       " 'abhik': 140,\n",
       " 'abhinav': 141,\n",
       " 'abhiruk': 142,\n",
       " 'abhishek': 143,\n",
       " 'abhradeep': 144,\n",
       " 'abhyankar': 145,\n",
       " 'abi': 146,\n",
       " 'abid': 147,\n",
       " 'abigail': 148,\n",
       " 'abii': 149,\n",
       " 'abil': 150,\n",
       " 'abilti': 151,\n",
       " 'abingdon': 152,\n",
       " 'abishanka': 153,\n",
       " 'abiteboul': 154,\n",
       " 'abj': 155,\n",
       " 'abjad': 156,\n",
       " 'abk': 157,\n",
       " 'abka': 158,\n",
       " 'abl': 159,\n",
       " 'ablat': 160,\n",
       " 'ablayev': 161,\n",
       " 'abley': 162,\n",
       " 'abli': 163,\n",
       " 'abm': 164,\n",
       " 'abmash': 165,\n",
       " 'abn': 166,\n",
       " 'abnorm': 167,\n",
       " 'abolhasan': 168,\n",
       " 'abolish': 169,\n",
       " 'abor': 170,\n",
       " 'aboratori': 171,\n",
       " 'abordagem': 172,\n",
       " 'abort': 173,\n",
       " 'abortship': 174,\n",
       " 'abouelleil': 175,\n",
       " 'aboulnasr': 176,\n",
       " 'abound': 177,\n",
       " 'aboutorab': 178,\n",
       " 'aboutp': 179,\n",
       " 'aboutt': 180,\n",
       " 'abovement': 181,\n",
       " 'abovepdefinit': 182,\n",
       " 'abovepmean': 183,\n",
       " 'abowd': 184,\n",
       " 'abox': 185,\n",
       " 'abp': 186,\n",
       " 'abpd': 187,\n",
       " 'abpl': 188,\n",
       " 'abpq': 189,\n",
       " 'abqp': 190,\n",
       " 'abr': 191,\n",
       " 'abraham': 192,\n",
       " 'abrahamson': 193,\n",
       " 'abram': 194,\n",
       " 'abramovich': 195,\n",
       " 'abramowitz': 196,\n",
       " 'abramski': 197,\n",
       " 'abramson': 198,\n",
       " 'abrego': 199,\n",
       " 'abreu': 200,\n",
       " 'abrial': 201,\n",
       " 'abridg': 202,\n",
       " 'abril': 203,\n",
       " 'abroad': 204,\n",
       " 'abrupt': 205,\n",
       " 'abruptli': 206,\n",
       " 'abrusci': 207,\n",
       " 'abrutyn': 208,\n",
       " 'abruzzi': 209,\n",
       " 'absb': 210,\n",
       " 'abscha': 211,\n",
       " 'abscissa': 212,\n",
       " 'abscoeff': 213,\n",
       " 'absenc': 214,\n",
       " 'absent': 215,\n",
       " 'absi': 216,\n",
       " 'absil': 217,\n",
       " 'absolut': 218,\n",
       " 'absolutecw': 219,\n",
       " 'absoluteleximin': 220,\n",
       " 'absorb': 221,\n",
       " 'absorpt': 222,\n",
       " 'absp': 223,\n",
       " 'abstent': 224,\n",
       " 'abstract': 225,\n",
       " 'abstractli': 226,\n",
       " 'abstractlinkedlist': 227,\n",
       " 'abstractorderedmapdecor': 228,\n",
       " 'abstractst': 229,\n",
       " 'abstrus': 230,\n",
       " 'absurd': 231,\n",
       " 'absurdo': 232,\n",
       " 'absurdum': 233,\n",
       " 'abt': 234,\n",
       " 'abtahi': 235,\n",
       " 'abu': 236,\n",
       " 'abualrub': 237,\n",
       " 'abubakr': 238,\n",
       " 'abugida': 239,\n",
       " 'abujarad': 240,\n",
       " 'abund': 241,\n",
       " 'abundanceecosystem': 242,\n",
       " 'abundanceproject': 243,\n",
       " 'abundantli': 244,\n",
       " 'abuot': 245,\n",
       " 'abur': 246,\n",
       " 'abus': 247,\n",
       " 'abut': 248,\n",
       " 'abuz': 249,\n",
       " 'abuzaid': 250,\n",
       " 'abv': 251,\n",
       " 'abw': 252,\n",
       " 'abx': 253,\n",
       " 'abya': 254,\n",
       " 'abz': 255,\n",
       " 'ac': 256,\n",
       " 'aca': 257,\n",
       " 'acad': 258,\n",
       " 'academ': 259,\n",
       " 'academi': 260,\n",
       " 'academia': 261,\n",
       " 'academiai': 262,\n",
       " 'academician': 263,\n",
       " 'acadmi': 264,\n",
       " 'acampora': 265,\n",
       " 'acapulco': 266,\n",
       " 'acar': 267,\n",
       " 'acarchau': 268,\n",
       " 'acaus': 269,\n",
       " 'acb': 270,\n",
       " 'acbc': 271,\n",
       " 'acbct': 272,\n",
       " 'acbd': 273,\n",
       " 'acbt': 274,\n",
       " 'acc': 275,\n",
       " 'accademia': 276,\n",
       " 'accardi': 277,\n",
       " 'accd': 278,\n",
       " 'accdhyp': 279,\n",
       " 'accel': 280,\n",
       " 'acceler': 281,\n",
       " 'acceleromet': 282,\n",
       " 'accent': 283,\n",
       " 'accentu': 284,\n",
       " 'accentur': 285,\n",
       " 'accept': 286,\n",
       " 'acceptor': 287,\n",
       " 'acces': 288,\n",
       " 'access': 289,\n",
       " 'accessiblilti': 290,\n",
       " 'accessor': 291,\n",
       " 'accessori': 292,\n",
       " 'accg': 293,\n",
       " 'accghyp': 294,\n",
       " 'acchyp': 295,\n",
       " 'acci': 296,\n",
       " 'accid': 297,\n",
       " 'accident': 298,\n",
       " 'accl': 299,\n",
       " 'acclaim': 300,\n",
       " 'accn': 301,\n",
       " 'acco': 302,\n",
       " 'accomazzi': 303,\n",
       " 'accommod': 304,\n",
       " 'accomod': 305,\n",
       " 'accompani': 306,\n",
       " 'accomplic': 307,\n",
       " 'accomplish': 308,\n",
       " 'accor': 309,\n",
       " 'accord': 310,\n",
       " 'accordi': 311,\n",
       " 'accordingli': 312,\n",
       " 'account': 313,\n",
       " 'accountmanag': 314,\n",
       " 'accountmanagerservic': 315,\n",
       " 'accoust': 316,\n",
       " 'accratef': 317,\n",
       " 'accratefhyp': 318,\n",
       " 'accrateg': 319,\n",
       " 'accrateghyp': 320,\n",
       " 'accratem': 321,\n",
       " 'accrel': 322,\n",
       " 'accru': 323,\n",
       " 'acct': 324,\n",
       " 'accuari': 325,\n",
       " 'accumul': 326,\n",
       " 'accur': 327,\n",
       " 'accuraci': 328,\n",
       " 'accus': 329,\n",
       " 'accv': 330,\n",
       " 'accx': 331,\n",
       " 'acd': 332,\n",
       " 'acdb': 333,\n",
       " 'ace': 334,\n",
       " 'acebro': 335,\n",
       " 'acebron': 336,\n",
       " 'acecomplet': 337,\n",
       " 'acemog': 338,\n",
       " 'acemoglu': 339,\n",
       " 'acentr': 340,\n",
       " 'acerbi': 341,\n",
       " 'acerca': 342,\n",
       " 'acero': 343,\n",
       " 'aceska': 344,\n",
       " 'aceto': 345,\n",
       " 'acevedo': 346,\n",
       " 'acf': 347,\n",
       " 'acfor': 348,\n",
       " 'acg': 349,\n",
       " 'acgt': 350,\n",
       " 'ach': 351,\n",
       " 'achan': 352,\n",
       " 'achanta': 353,\n",
       " 'achar': 354,\n",
       " 'achara': 355,\n",
       " 'achari': 356,\n",
       " 'acharya': 357,\n",
       " 'acheiev': 358,\n",
       " 'achi': 359,\n",
       " 'achieiv': 360,\n",
       " 'achiev': 361,\n",
       " 'achievabil': 362,\n",
       " 'achievabiliy': 363,\n",
       " 'achievabl': 364,\n",
       " 'achievebl': 365,\n",
       " 'achievedpsfrag': 366,\n",
       " 'achil': 367,\n",
       " 'achillea': 368,\n",
       " 'achilleo': 369,\n",
       " 'achim': 370,\n",
       " 'achin': 371,\n",
       " 'achiv': 372,\n",
       " 'achliopta': 373,\n",
       " 'achrekar': 374,\n",
       " 'achterberg': 375,\n",
       " 'achtergracht': 376,\n",
       " 'aci': 377,\n",
       " 'acial': 378,\n",
       " 'acid': 379,\n",
       " 'acifar': 380,\n",
       " 'acin': 381,\n",
       " 'acit': 382,\n",
       " 'aciv': 383,\n",
       " 'ack': 384,\n",
       " 'ackerman': 385,\n",
       " 'ackermann': 386,\n",
       " 'acket': 387,\n",
       " 'acki': 388,\n",
       " 'ackley': 389,\n",
       " 'acklm': 390,\n",
       " 'acknowledg': 391,\n",
       " 'acknowlegd': 392,\n",
       " 'ackoff': 393,\n",
       " 'ackowledg': 394,\n",
       " 'ackstrom': 395,\n",
       " 'acl': 396,\n",
       " 'aclf': 397,\n",
       " 'aclust': 398,\n",
       " 'acm': 399,\n",
       " 'acml': 400,\n",
       " 'acmsiam': 401,\n",
       " 'acn': 402,\n",
       " 'aco': 403,\n",
       " 'acolor': 404,\n",
       " 'acompi': 405,\n",
       " 'acomprehens': 406,\n",
       " 'aconsist': 407,\n",
       " 'aconst': 408,\n",
       " 'acontrol': 409,\n",
       " 'acopf': 410,\n",
       " 'acori': 411,\n",
       " 'acou': 412,\n",
       " 'acount': 413,\n",
       " 'acoust': 414,\n",
       " 'acp': 415,\n",
       " 'acpa': 416,\n",
       " 'acpj': 417,\n",
       " 'acqpa': 418,\n",
       " 'acquaint': 419,\n",
       " 'acquir': 420,\n",
       " 'acquisit': 421,\n",
       " 'acquisti': 422,\n",
       " 'acr': 423,\n",
       " 'acrf': 424,\n",
       " 'acri': 425,\n",
       " 'acro': 426,\n",
       " 'acrocel': 427,\n",
       " 'acronym': 428,\n",
       " 'across': 429,\n",
       " 'acsac': 430,\n",
       " 'acsd': 431,\n",
       " 'acsm': 432,\n",
       " 'acssc': 433,\n",
       " 'acsysimp': 434,\n",
       " 'act': 435,\n",
       " 'acta': 436,\n",
       " 'acteris': 437,\n",
       " 'acti': 438,\n",
       " 'actin': 439,\n",
       " 'action': 440,\n",
       " 'actionlabel': 441,\n",
       " 'actionrecord': 442,\n",
       " 'activ': 443,\n",
       " 'activationbas': 444,\n",
       " 'activationdesc': 445,\n",
       " 'activationgroup': 446,\n",
       " 'activationsystem': 447,\n",
       " 'activebodi': 448,\n",
       " 'activeset': 449,\n",
       " 'activist': 450,\n",
       " 'activit': 451,\n",
       " 'activityaspect': 452,\n",
       " 'activityedg': 453,\n",
       " 'activityedgeaspect': 454,\n",
       " 'activityedgeimpl': 455,\n",
       " 'activityimpl': 456,\n",
       " 'activitynet': 457,\n",
       " 'activitynod': 458,\n",
       " 'activitynodeactivationgroup': 459,\n",
       " 'activitynodeaspect': 460,\n",
       " 'activitynodeimpl': 461,\n",
       " 'activityparameternod': 462,\n",
       " 'activityu': 463,\n",
       " 'activityx': 464,\n",
       " 'acton': 465,\n",
       " 'actonto': 466,\n",
       " 'actor': 467,\n",
       " 'actorfoundri': 468,\n",
       " 'actorref': 469,\n",
       " 'actpbq': 470,\n",
       " 'actpq': 471,\n",
       " 'actpqq': 472,\n",
       " 'actq': 473,\n",
       " 'actscr': 474,\n",
       " 'actsh': 475,\n",
       " 'actshr': 476,\n",
       " 'actsi': 477,\n",
       " 'actslo': 478,\n",
       " 'actt': 479,\n",
       " 'actu': 480,\n",
       " 'actual': 481,\n",
       " 'actualis': 482,\n",
       " 'actualment': 483,\n",
       " 'actuari': 484,\n",
       " 'actuat': 485,\n",
       " 'acu': 486,\n",
       " 'acustica': 487,\n",
       " 'acut': 488,\n",
       " 'acvt': 489,\n",
       " 'acycl': 490,\n",
       " 'acyclicli': 491,\n",
       " 'acyl': 492,\n",
       " 'acz': 493,\n",
       " 'aczel': 494,\n",
       " 'ad': 495,\n",
       " 'ada': 496,\n",
       " 'adaboost': 497,\n",
       " 'adachi': 498,\n",
       " 'adadelta': 499,\n",
       " 'adadi': 500,\n",
       " 'adag': 501,\n",
       " 'adalbert': 502,\n",
       " 'adalf': 503,\n",
       " 'adalsteinsson': 504,\n",
       " 'adam': 505,\n",
       " 'adamatzki': 506,\n",
       " 'adamczak': 507,\n",
       " 'adamek': 508,\n",
       " 'adami': 509,\n",
       " 'adamsbashforth': 510,\n",
       " 'adamson': 511,\n",
       " 'adao': 512,\n",
       " 'adap': 513,\n",
       " 'adapt': 514,\n",
       " 'adapta': 515,\n",
       " 'adaptationcorrect': 516,\n",
       " 'adaptiverobust': 517,\n",
       " 'adar': 518,\n",
       " 'adaricheva': 519,\n",
       " 'adat': 520,\n",
       " 'adavani': 521,\n",
       " 'adc': 522,\n",
       " 'adcock': 523,\n",
       " 'adcol': 524,\n",
       " 'add': 525,\n",
       " 'addabbo': 526,\n",
       " 'addad': 527,\n",
       " 'addaiu': 528,\n",
       " 'addario': 529,\n",
       " 'addcandrul': 530,\n",
       " 'addclocklisten': 531,\n",
       " 'addend': 532,\n",
       " 'addendum': 533,\n",
       " 'adder': 534,\n",
       " 'addflip': 535,\n",
       " 'addi': 536,\n",
       " 'addict': 537,\n",
       " 'addis': 538,\n",
       " 'addison': 539,\n",
       " 'addisonwesley': 540,\n",
       " 'addisson': 541,\n",
       " 'addit': 542,\n",
       " 'additio': 543,\n",
       " 'additiveacross': 544,\n",
       " 'additiveerror': 545,\n",
       " 'additiven': 546,\n",
       " 'addiu': 547,\n",
       " 'addnod': 548,\n",
       " 'addon': 549,\n",
       " 'addr': 550,\n",
       " 'addrandomappl': 551,\n",
       " 'address': 552,\n",
       " 'addresse': 553,\n",
       " 'addscor': 554,\n",
       " 'addscorei': 555,\n",
       " 'addtion': 556,\n",
       " 'addu': 557,\n",
       " 'adduc': 558,\n",
       " 'ade': 559,\n",
       " 'adecod': 560,\n",
       " 'adel': 561,\n",
       " 'adelaid': 562,\n",
       " 'adelgren': 563,\n",
       " 'adelin': 564,\n",
       " 'adelman': 565,\n",
       " 'adelmann': 566,\n",
       " 'adelson': 567,\n",
       " 'aden': 568,\n",
       " 'adenin': 569,\n",
       " 'adequ': 570,\n",
       " 'adequaci': 571,\n",
       " 'ader': 572,\n",
       " 'aderiv': 573,\n",
       " 'adesnik': 574,\n",
       " 'adesso': 575,\n",
       " 'adetunji': 576,\n",
       " 'adewol': 577,\n",
       " 'adford': 578,\n",
       " 'adfsdca': 579,\n",
       " 'adg': 580,\n",
       " 'adga': 581,\n",
       " 'adham': 582,\n",
       " 'adher': 583,\n",
       " 'adhes': 584,\n",
       " 'adhikari': 585,\n",
       " 'adhoc': 586,\n",
       " 'adhu': 587,\n",
       " 'adi': 588,\n",
       " 'adiabat': 589,\n",
       " 'adiac': 590,\n",
       " 'adibi': 591,\n",
       " 'adic': 592,\n",
       " 'adida': 593,\n",
       " 'adifor': 594,\n",
       " 'adigit': 595,\n",
       " 'adijac': 596,\n",
       " 'adilson': 597,\n",
       " 'adimat': 598,\n",
       " 'adio': 599,\n",
       " 'adipos': 600,\n",
       " 'adiqu': 601,\n",
       " 'adish': 602,\n",
       " 'adistm': 603,\n",
       " 'adit': 604,\n",
       " 'aditya': 605,\n",
       " 'adiwena': 606,\n",
       " 'adj': 607,\n",
       " 'adjac': 608,\n",
       " 'adjacent': 609,\n",
       " 'adjacentclos': 610,\n",
       " 'adjacentclosur': 611,\n",
       " 'adject': 612,\n",
       " 'adji': 613,\n",
       " 'adjnoun': 614,\n",
       " 'adjoin': 615,\n",
       " 'adjoint': 616,\n",
       " 'adjud': 617,\n",
       " 'adjug': 618,\n",
       " 'adjunct': 619,\n",
       " 'adjust': 620,\n",
       " 'adjustp': 621,\n",
       " 'adk': 622,\n",
       " 'adl': 623,\n",
       " 'adlakha': 624,\n",
       " 'adleman': 625,\n",
       " 'adler': 626,\n",
       " 'adma': 627,\n",
       " 'admetox': 628,\n",
       " 'admin': 629,\n",
       " 'administ': 630,\n",
       " 'administr': 631,\n",
       " 'admir': 632,\n",
       " 'admira': 633,\n",
       " 'admiss': 634,\n",
       " 'admistr': 635,\n",
       " 'admit': 636,\n",
       " 'admitt': 637,\n",
       " 'admittedli': 638,\n",
       " 'admm': 639,\n",
       " 'adn': 640,\n",
       " 'adnhztyclxk': 641,\n",
       " 'adnot': 642,\n",
       " 'ado': 643,\n",
       " 'adob': 644,\n",
       " 'adol': 645,\n",
       " 'adolesc': 646,\n",
       " 'adolfo': 647,\n",
       " 'adom': 648,\n",
       " 'adomaviciu': 649,\n",
       " 'adopt': 650,\n",
       " 'adorn': 651,\n",
       " 'adp': 652,\n",
       " 'adpk': 653,\n",
       " 'adpoint': 654,\n",
       " 'adpt': 655,\n",
       " 'adptiv': 656,\n",
       " 'adq': 657,\n",
       " 'adr': 658,\n",
       " 'adress': 659,\n",
       " 'adresses': 660,\n",
       " 'adria': 661,\n",
       " 'adrian': 662,\n",
       " 'adriana': 663,\n",
       " 'adriano': 664,\n",
       " 'adrien': 665,\n",
       " 'adsaf': 666,\n",
       " 'adsolv': 667,\n",
       " 'adsorpt': 668,\n",
       " 'adt': 669,\n",
       " 'adtk': 670,\n",
       " 'adu': 671,\n",
       " 'adult': 672,\n",
       " 'adulthood': 673,\n",
       " 'adusumilli': 674,\n",
       " 'adv': 675,\n",
       " 'advanc': 676,\n",
       " 'advancepoint': 677,\n",
       " 'advancingfront': 678,\n",
       " 'advantag': 679,\n",
       " 'advcdh': 680,\n",
       " 'advclass': 681,\n",
       " 'advdummi': 682,\n",
       " 'advect': 683,\n",
       " 'advectivediffus': 684,\n",
       " 'advent': 685,\n",
       " 'adventur': 686,\n",
       " 'advers': 687,\n",
       " 'adversari': 688,\n",
       " 'advertis': 689,\n",
       " 'advic': 690,\n",
       " 'advis': 691,\n",
       " 'advisor': 692,\n",
       " 'advisori': 693,\n",
       " 'advoc': 694,\n",
       " 'advocaci': 695,\n",
       " 'advol': 696,\n",
       " 'adwait': 697,\n",
       " 'adword': 698,\n",
       " 'adx': 699,\n",
       " 'adz': 700,\n",
       " 'ae': 701,\n",
       " 'aeb': 702,\n",
       " 'aecor': 703,\n",
       " 'aee': 704,\n",
       " 'aef': 705,\n",
       " 'aegean': 706,\n",
       " 'aegi': 707,\n",
       " 'aei': 708,\n",
       " 'aej': 709,\n",
       " 'aejb': 710,\n",
       " 'aek': 711,\n",
       " 'aekd': 712,\n",
       " 'ael': 713,\n",
       " 'aella': 714,\n",
       " 'aem': 715,\n",
       " 'aen': 716,\n",
       " 'aenc': 717,\n",
       " 'aeolian': 718,\n",
       " 'aeolu': 719,\n",
       " 'aep': 720,\n",
       " 'aeq': 721,\n",
       " 'aequation': 722,\n",
       " 'aeqxagfbnlulzhpzmpynip': 723,\n",
       " 'aerial': 724,\n",
       " 'aero': 725,\n",
       " 'aeroacoust': 726,\n",
       " 'aerodynam': 727,\n",
       " 'aerofoil': 728,\n",
       " 'aeron': 729,\n",
       " 'aeronaut': 730,\n",
       " 'aeroplan': 731,\n",
       " 'aerosp': 732,\n",
       " 'aerospac': 733,\n",
       " 'aert': 734,\n",
       " 'aesn': 735,\n",
       " 'aesop': 736,\n",
       " 'aesthet': 737,\n",
       " 'aeterna': 738,\n",
       " 'aexp': 739,\n",
       " 'aexppol': 740,\n",
       " 'aext': 741,\n",
       " 'aeyel': 742,\n",
       " 'af': 743,\n",
       " 'afa': 744,\n",
       " 'afactor': 745,\n",
       " 'afanasiev': 746,\n",
       " 'afanasyev': 747,\n",
       " 'afar': 748,\n",
       " 'afc': 749,\n",
       " 'afcet': 750,\n",
       " 'afcetsmf': 751,\n",
       " 'afek': 752,\n",
       " 'afem': 753,\n",
       " 'aff': 754,\n",
       " 'affair': 755,\n",
       " 'affdim': 756,\n",
       " 'affect': 757,\n",
       " 'affection': 758,\n",
       " 'affer': 759,\n",
       " 'affil': 760,\n",
       " 'affili': 761,\n",
       " 'affin': 762,\n",
       " 'affirm': 763,\n",
       " 'affix': 764,\n",
       " 'afflict': 765,\n",
       " 'affonso': 766,\n",
       " 'afford': 767,\n",
       " 'affymetrix': 768,\n",
       " 'afh': 769,\n",
       " 'afi': 770,\n",
       " 'afield': 771,\n",
       " 'afip': 772,\n",
       " 'afivo': 773,\n",
       " 'afk': 774,\n",
       " 'afl': 775,\n",
       " 'aflalo': 776,\n",
       " 'aflw': 777,\n",
       " 'afm': 778,\n",
       " 'afma': 779,\n",
       " 'afmm': 780,\n",
       " 'afn': 781,\n",
       " 'afonso': 782,\n",
       " 'afor': 783,\n",
       " 'aforement': 784,\n",
       " 'aforesaid': 785,\n",
       " 'afosr': 786,\n",
       " 'afosrfa': 787,\n",
       " 'afoul': 788,\n",
       " 'afp': 789,\n",
       " 'afra': 790,\n",
       " 'afrabandpey': 791,\n",
       " 'afrabandpeyb': 792,\n",
       " 'afraid': 793,\n",
       " 'afrati': 794,\n",
       " 'afresh': 795,\n",
       " 'afriat': 796,\n",
       " 'africa': 797,\n",
       " 'african': 798,\n",
       " 'africon': 799,\n",
       " 'afrl': 800,\n",
       " 'afro': 801,\n",
       " 'afschem': 802,\n",
       " 'afshang': 803,\n",
       " 'afshar': 804,\n",
       " 'afshin': 805,\n",
       " 'aft': 806,\n",
       " 'aftan': 807,\n",
       " 'afteract': 808,\n",
       " 'aftermath': 809,\n",
       " 'afternoon': 810,\n",
       " 'aftershock': 811,\n",
       " 'aftersremov': 812,\n",
       " 'afterward': 813,\n",
       " 'afterwardsin': 814,\n",
       " 'afterword': 815,\n",
       " 'aftn': 816,\n",
       " 'afu': 817,\n",
       " 'afuk': 818,\n",
       " 'afw': 819,\n",
       " 'afz': 820,\n",
       " 'ag': 821,\n",
       " 'aga': 822,\n",
       " 'agadzanyan': 823,\n",
       " 'agafonov': 824,\n",
       " 'agaian': 825,\n",
       " 'agapi': 826,\n",
       " 'agapito': 827,\n",
       " 'agaraj': 828,\n",
       " 'agaraja': 829,\n",
       " 'agarawala': 830,\n",
       " 'agard': 831,\n",
       " 'agarw': 832,\n",
       " 'agarwala': 833,\n",
       " 'agatho': 834,\n",
       " 'agatz': 835,\n",
       " 'agazin': 836,\n",
       " 'agc': 837,\n",
       " 'agct': 838,\n",
       " 'agcta': 839,\n",
       " 'agda': 840,\n",
       " 'age': 841,\n",
       " 'ageev': 842,\n",
       " 'ageman': 843,\n",
       " 'agenc': 844,\n",
       " 'agenda': 845,\n",
       " 'agent': 846,\n",
       " 'agentbas': 847,\n",
       " 'agentmodel': 848,\n",
       " 'agentp': 849,\n",
       " 'agentservicetheori': 850,\n",
       " 'agentspecif': 851,\n",
       " 'ager': 852,\n",
       " 'agesh': 853,\n",
       " 'aget': 854,\n",
       " 'agevari': 855,\n",
       " 'agfa': 856,\n",
       " 'agg': 857,\n",
       " 'aggarw': 858,\n",
       " 'aggelo': 859,\n",
       " 'agglom': 860,\n",
       " 'agglomer': 861,\n",
       " 'agglomera': 862,\n",
       " 'aggoun': 863,\n",
       " 'aggrav': 864,\n",
       " 'aggreg': 865,\n",
       " 'aggregat': 866,\n",
       " 'aggres': 867,\n",
       " 'aggress': 868,\n",
       " 'aggverifi': 869,\n",
       " 'agh': 870,\n",
       " 'agha': 871,\n",
       " 'aghabozorgi': 872,\n",
       " 'aghae': 873,\n",
       " 'aghannan': 874,\n",
       " 'aghaven': 875,\n",
       " 'aghu': 876,\n",
       " 'agi': 877,\n",
       " 'agichtein': 878,\n",
       " 'agil': 879,\n",
       " 'agiomyrgiannaki': 880,\n",
       " 'agit': 881,\n",
       " 'agiu': 882,\n",
       " 'agiv': 883,\n",
       " 'agl': 884,\n",
       " 'agliano': 885,\n",
       " 'aglidereiread': 886,\n",
       " 'agliderseiread': 887,\n",
       " 'aglio': 888,\n",
       " 'agm': 889,\n",
       " 'agmon': 890,\n",
       " 'agn': 891,\n",
       " 'agnar': 892,\n",
       " 'agneessen': 893,\n",
       " 'agner': 894,\n",
       " 'agnieszka': 895,\n",
       " 'agnihotri': 896,\n",
       " 'agnost': 897,\n",
       " 'ago': 898,\n",
       " 'agol': 899,\n",
       " 'agon': 900,\n",
       " 'agonist': 901,\n",
       " 'agostino': 902,\n",
       " 'agotn': 903,\n",
       " 'agp': 904,\n",
       " 'agr': 905,\n",
       " 'agrachev': 906,\n",
       " 'agraw': 907,\n",
       " 'agre': 908,\n",
       " 'agreement': 909,\n",
       " 'agresti': 910,\n",
       " 'agribio': 911,\n",
       " 'agricultur': 912,\n",
       " 'agroparistech': 913,\n",
       " 'agrophi': 914,\n",
       " 'agt': 915,\n",
       " 'aguayo': 916,\n",
       " 'aguc': 917,\n",
       " 'agudelo': 918,\n",
       " 'agueda': 919,\n",
       " 'aguiar': 920,\n",
       " 'aguilar': 921,\n",
       " 'aguilera': 922,\n",
       " 'aguillo': 923,\n",
       " 'aguirr': 924,\n",
       " 'agullo': 925,\n",
       " 'agustin': 926,\n",
       " 'agustino': 927,\n",
       " 'agv': 928,\n",
       " 'agvm': 929,\n",
       " 'ah': 930,\n",
       " 'aha': 931,\n",
       " 'ahai': 932,\n",
       " 'ahajan': 933,\n",
       " 'aharia': 934,\n",
       " 'aharon': 935,\n",
       " 'aharoni': 936,\n",
       " 'aharonov': 937,\n",
       " 'ahasanun': 938,\n",
       " 'ahb': 939,\n",
       " 'ahc': 940,\n",
       " 'ahdki': 941,\n",
       " 'ahe': 942,\n",
       " 'ahead': 943,\n",
       " 'ahel': 944,\n",
       " 'ahern': 945,\n",
       " 'ahh': 946,\n",
       " 'ahhi': 947,\n",
       " 'ahi': 948,\n",
       " 'ahigh': 949,\n",
       " 'ahipasaoglu': 950,\n",
       " 'ahk': 951,\n",
       " 'ahl': 952,\n",
       " 'ahlgren': 953,\n",
       " 'ahlswed': 954,\n",
       " 'ahm': 955,\n",
       " 'ahmad': 956,\n",
       " 'ahmadabadi': 957,\n",
       " 'ahmadi': 958,\n",
       " 'ahmadia': 959,\n",
       " 'ahmadian': 960,\n",
       " 'ahmet': 961,\n",
       " 'ahn': 962,\n",
       " 'aho': 963,\n",
       " 'aholt': 964,\n",
       " 'ahonen': 965,\n",
       " 'ahoney': 966,\n",
       " 'ahp': 967,\n",
       " 'ahpi': 968,\n",
       " 'ahrc': 969,\n",
       " 'ahren': 970,\n",
       " 'ahrweil': 971,\n",
       " 'ahuja': 972,\n",
       " 'ahv': 973,\n",
       " 'ai': 974,\n",
       " 'aiaa': 975,\n",
       " 'aib': 976,\n",
       " 'aibo': 977,\n",
       " 'aibx': 978,\n",
       " 'aic': 979,\n",
       " 'aicc': 980,\n",
       " 'aicedo': 981,\n",
       " 'aich': 982,\n",
       " 'aichernig': 983,\n",
       " 'aichholz': 984,\n",
       " 'aichi': 985,\n",
       " 'aid': 986,\n",
       " 'aida': 987,\n",
       " 'aiden': 988,\n",
       " 'aidwyc': 989,\n",
       " 'aie': 990,\n",
       " 'aiee': 991,\n",
       " 'aiello': 992,\n",
       " 'aier': 993,\n",
       " 'aif': 994,\n",
       " 'aifanti': 995,\n",
       " 'aifman': 996,\n",
       " 'aig': 997,\n",
       " 'aigd': 998,\n",
       " 'aign': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'BoW1.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))\n",
    "#loaded_model = pickle.load(open('BoW1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(vocabulary - words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
