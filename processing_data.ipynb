{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/cmejia3/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(['punkt','stopwords','wordnet','words'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from nltk.corpus import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path =  Path(\"~\").expanduser().resolve()\n",
    "#base_path = Path.cwd().expanduser().resolve()\n",
    "input_file_path  = base_path / 'datasets/papers-txt/'\n",
    "#datasetOut =base_path / \"datasets/salidas_procesamiento/\"\n",
    "#datasetOut_freq = base_path / \"datasets/salidas_freq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cmejia3/datasets/papers-txt')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileSize(fileIn):\n",
    "    size=os.stat(fileIn).st_size\n",
    "    return size\n",
    "\n",
    "def wordsCount(contenido):\n",
    "    totWwords=contenido.split()\n",
    "    return len(totWwords)\n",
    "\n",
    "def cleanWordCount(contenido):\n",
    "    contenido =re.sub('(f|ht)tp(s?)://(.*)[.][a-z]+',' ',contenido) # Eliminar las URL\n",
    "    #contenido =re.sub('REFERENCES (\\S|\\w)+',' ',contenido) # Eliminar la bibliografia\n",
    "    contenido =re.sub('[a-zA-Z0-9.?{}]+@\\w+\\.\\w+.\\w*','',contenido) # Eliminar los correos\n",
    "    contenido =re.sub('\\[[a-zA-Z0-9\\,\\. ]+\\]','',contenido) # Eliminar cualquier contenido entre corchetes\n",
    "    contenido =re.sub('\\([a-zA-Z0-9\\,\\.\\- ]+\\)',' ',contenido) # Eliminar cualquier contenido entre paréntesis\n",
    "    contenido =re.sub('((et al\\.)|(i\\.i\\.d\\.)|(i\\.e\\.)|\\-|\\'|\\’|\\`)','',contenido) # Eliminar abreviaciones, apostrofes y guion\n",
    "    #contenido =re.sub('(f|F)igure [0-9]+.[0-9]',' ',contenido) # Eliminar Figure\n",
    "    contenido =re.sub('[^a-zA-Z_á\\éíóúà\\èìòùäëïöü\\s]','',contenido) # Eliminar caracteres que no sean: letra, número o vocales acentuadas\n",
    "    contenido =re.sub(' +',' ',contenido) # Eliminar espacios en blanco\n",
    "    contenido =re.sub('(a-z|A-Z){1,1}','',contenido) # Eliminar palabras o números de un caracter de longitud   \n",
    "    #contenido =re.sub('[^A-Za-z0-9.,_%+-\\(\\)\\[\\]\\´\\'\\`]',' ',contenido)\n",
    "    #contenido =re.sub('\\[(0-9)+\\]',' ',contenido)    \n",
    "    #totWordDepurado = Counter(map(str, contenido.split()))\n",
    "    #outputFile= open(datasetOut, 'w', encoding='UTF-8')\n",
    "    #outputFile.write(contenido)\n",
    "    #outputFile.close()\n",
    "    totWwords=contenido.split()\n",
    "    setWords = set(totWwords)\n",
    "    #print(\"Total de palabras {}\".format(len(totWwords)))\n",
    "    #print(\"Total de palabras después del pre-procesamiento: {}\".format(totWordDepurado))\n",
    "    return len(totWwords),contenido,setWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_ratio(input):\n",
    "    lang_ratio = {}\n",
    "    tokens = wordpunct_tokenize(input)\n",
    "    words = [word.lower() for word in tokens]\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        word_set = set(words)\n",
    "        common_elements = word_set.intersection(stopwords_set)\n",
    "        lang_ratio[language] = len(common_elements)\n",
    "    return lang_ratio\n",
    "\n",
    "def detect_language(input):\n",
    "    ratios = lang_ratio(input)\n",
    "    language = max(ratios, key = ratios.get)\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de las stop words que serán eliminadas ya que no aportan valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras que considermos StopWords que no estan incluidas en el conjunto descargado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listoStopWords = ['www','https','html','figure', 'chapter','abbcbccabcabcabcabcbcbabacba']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añaden las palabras que consideramos al conjunto principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.extend(listoStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función principal procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_files(texto,stopWords):\n",
    "    \n",
    "    #Quitar todos los acentos\n",
    "    #texto = unidecode.unidecode(texto)\n",
    "    \n",
    "    #Quitar todos los caracteres especiales\n",
    "    texto = re.sub('[^A-Za-z0-9]+',' ',texto)\n",
    "    \n",
    "    #Pasar todo a minisculas\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    #Tokenizar\n",
    "    tokens = texto.split()\n",
    "    \n",
    "    #Variable que guarda el año en el que estamos que es el limite superior de los números que no se van a eliminar\n",
    "    currentYear = int(dt.datetime.now().year)\n",
    "    \n",
    "    #Verificar que las palabras tengan más de un caracter, que además sean solo sean letras\n",
    "    # o si son números que esten entre un rango que sea admisible para no eliminar información de año que se mencione en los artículos\n",
    "    # y finalmente que no sean palabras que estan en el dicccionario de stopwords.\n",
    "    \n",
    "    #tokens = [w for w in tokens if (len(w)>1)&(w.isalpha() or (w.isnumeric() and int(w)>=1800 and int(w)<=currentYear))&(w not in stopWords)]\n",
    "    tokens = [w for w in tokens if (len(w)>1)&(w.isalpha())&(w not in stopWords)]\n",
    "\n",
    "    #Stemmer\n",
    "    ps = PorterStemmer() \n",
    "    tokens = [ps.stem(w) for w in tokens]\n",
    "    \n",
    "    #Lematización\n",
    "    word_net_lemmatizar = WordNetLemmatizer()\n",
    "\n",
    "    tokens = [word_net_lemmatizar.lemmatize(w, pos = \"v\") for w in tokens]\n",
    "    \n",
    "    #Se retorna el texto nuevamente en un solo string luego de ser procesado\n",
    "    to_return = ' '.join(tokens)\n",
    "    \n",
    "    #Se retorna el vocabulario de cada documento\n",
    "    set_words = set(tokens)\n",
    "    \n",
    "    #Y la frecuencia de las palabras\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    return to_return,set_words,freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de los conjuntos:\n",
    "\n",
    "    - Vocabulary: el conjunto de todas las palabras que contienen los documentos\n",
    "    - results_text: la lista con los documentos ya organizados para construir el bag of words\n",
    "    - results_frecuency: información de cada documento de las palabras que contiene cuántas veces las contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "results_text = []\n",
    "results_frecuency = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german:/home/cmejia3/datasets/papers-txt/1508.02340.txt\n"
     ]
    }
   ],
   "source": [
    "fileSummary = \"CleanSummary.csv\"\n",
    "\n",
    "contenido= \"Archivo\" + \";\" + \"Tamaño(K)\" + \";\" + \"Cant Palabras Inicial\" + \";\" + \"Cant Palabras depuradas\"+ \";\" +\"Porc Limpieza\"+ \"\\n\"\n",
    "resultado = pd.DataFrame()\n",
    "indexFiles = []\n",
    "documents = []\n",
    "\n",
    "for f in input_file_path.glob('*.txt'):\n",
    "    #Peso del archivo\n",
    "    tmpSize=round(fileSize(f)/1014)\n",
    "    \n",
    "    #Lectura del archivo\n",
    "    input_file = open(f, \"r\", encoding = 'utf-8')\n",
    "    input_aux = input_file.read()\n",
    "    \n",
    "    #Cuenta de palabras iniciales\n",
    "    tmpWordsOri=wordsCount(input_aux)\n",
    "    #out = datasetOut / str(f).split('/')[-1]\n",
    "    \n",
    "    #Cuenta de palabras despues de la limpieza\n",
    "    tmpWordsEnd,text,setWord=cleanWordCount(input_aux)\n",
    "    #tmpPerClean=round((tmpWordsEnd/tmpWordsOri)*100)\n",
    "    \n",
    "    \n",
    "    #Detectando el idioma\n",
    "    aux = detect_language(text)\n",
    "    \n",
    "    if(aux == 'english'):\n",
    "        text_cleanned,set_words,freq = clean_files(text,stopWords)\n",
    "        #out2 = datasetOut / str(f).split('/')[-1]\n",
    "        documents.append(text_cleanned)\n",
    "        vocabulary = vocabulary.union(set_words)\n",
    "        results_text.append(text_cleanned)\n",
    "        indexFiles.append(str(f).split('/')[-1])\n",
    "        \n",
    "        #Escritura de los resultados del preprocesamiento\n",
    "        auxRes= pd.DataFrame({'Archivo': str(f).split('/')[-1], 'Tamaño(K)': [tmpSize], 'Cant Palabras Inicial': [tmpWordsOri],'Cant Palabras depuradas': [tmpWordsEnd],\"Vocabulario Inicial\":len(setWord),\"Vocabulario Final\":len(set_words)})\n",
    "        resultado = pd.concat([resultado, auxRes])\n",
    "        #break\n",
    "    else:\n",
    "        print(aux + ':' + str(f))\n",
    "\n",
    "resultado.to_csv(fileSummary, sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Archivo</th>\n",
       "      <th>Tamaño(K)</th>\n",
       "      <th>Cant Palabras Inicial</th>\n",
       "      <th>Cant Palabras depuradas</th>\n",
       "      <th>Vocabulario Inicial</th>\n",
       "      <th>Vocabulario Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1410.2670.txt</td>\n",
       "      <td>24</td>\n",
       "      <td>3926</td>\n",
       "      <td>3487</td>\n",
       "      <td>939</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1404.3626.txt</td>\n",
       "      <td>35</td>\n",
       "      <td>6247</td>\n",
       "      <td>4804</td>\n",
       "      <td>1072</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1405.0149.txt</td>\n",
       "      <td>34</td>\n",
       "      <td>7257</td>\n",
       "      <td>5066</td>\n",
       "      <td>928</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1409.2612.txt</td>\n",
       "      <td>21</td>\n",
       "      <td>4211</td>\n",
       "      <td>2924</td>\n",
       "      <td>503</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1511.00867.txt</td>\n",
       "      <td>72</td>\n",
       "      <td>14471</td>\n",
       "      <td>12191</td>\n",
       "      <td>1440</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Archivo  Tamaño(K)  Cant Palabras Inicial  Cant Palabras depuradas  \\\n",
       "0   1410.2670.txt         24                   3926                     3487   \n",
       "0   1404.3626.txt         35                   6247                     4804   \n",
       "0   1405.0149.txt         34                   7257                     5066   \n",
       "0   1409.2612.txt         21                   4211                     2924   \n",
       "0  1511.00867.txt         72                  14471                    12191   \n",
       "\n",
       "   Vocabulario Inicial  Vocabulario Final  \n",
       "0                  939                608  \n",
       "0                 1072                676  \n",
       "0                  928                619  \n",
       "0                  503                309  \n",
       "0                 1440                921  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101460"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import vstack,save_npz,load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construido el vocabulario podemos construir el bag of words, que se hace con la ayuda de la funcion CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",vocabulary =vocabulary , tokenizer = None, preprocessor = None, stop_words = 'english', max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101460"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('sparse_matrix.npz', train_data_features)\n",
    "#sparse_matrix = load_npz('sparse_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'aa': 1,\n",
       " 'aaa': 2,\n",
       " 'aaaa': 3,\n",
       " 'aaaaa': 4,\n",
       " 'aaaaaa': 5,\n",
       " 'aaaaaaa': 6,\n",
       " 'aaaaaaaaaaaaaaaababa': 7,\n",
       " 'aaaab': 8,\n",
       " 'aaaababa': 9,\n",
       " 'aaab': 10,\n",
       " 'aaabbbbaaccccaaaacbbbcccbb': 11,\n",
       " 'aaai': 12,\n",
       " 'aaaiaaaipaperview': 13,\n",
       " 'aaaibarri': 14,\n",
       " 'aaaimit': 15,\n",
       " 'aaak': 16,\n",
       " 'aab': 17,\n",
       " 'aaba': 18,\n",
       " 'aabaabc': 19,\n",
       " 'aabab': 20,\n",
       " 'aababa': 21,\n",
       " 'aababaababb': 22,\n",
       " 'aabb': 23,\n",
       " 'aabbb': 24,\n",
       " 'aabc': 25,\n",
       " 'aabcdaccaac': 26,\n",
       " 'aabort': 27,\n",
       " 'aacccgctcgt': 28,\n",
       " 'aaccept': 29,\n",
       " 'aachen': 30,\n",
       " 'aacn': 31,\n",
       " 'aaction': 32,\n",
       " 'aad': 33,\n",
       " 'aadbk': 34,\n",
       " 'aadom': 35,\n",
       " 'aaecc': 36,\n",
       " 'aaeccdoi': 37,\n",
       " 'aaem': 38,\n",
       " 'aaf': 39,\n",
       " 'aag': 40,\n",
       " 'aagaard': 41,\n",
       " 'aagea': 42,\n",
       " 'aah': 43,\n",
       " 'aai': 44,\n",
       " 'aaia': 45,\n",
       " 'aaihaa': 46,\n",
       " 'aain': 47,\n",
       " 'aaini': 48,\n",
       " 'aaixj': 49,\n",
       " 'aaj': 50,\n",
       " 'aajbb': 51,\n",
       " 'aak': 52,\n",
       " 'aakv': 53,\n",
       " 'aalbersberg': 54,\n",
       " 'aalborg': 55,\n",
       " 'aalcain': 56,\n",
       " 'aalg': 57,\n",
       " 'aall': 58,\n",
       " 'aalter': 59,\n",
       " 'aaltern': 60,\n",
       " 'aalto': 61,\n",
       " 'aam': 62,\n",
       " 'aama': 63,\n",
       " 'aamarkov': 64,\n",
       " 'aamodt': 65,\n",
       " 'aan': 66,\n",
       " 'aana': 67,\n",
       " 'aanderaa': 68,\n",
       " 'aank': 69,\n",
       " 'aanstad': 70,\n",
       " 'aanund': 71,\n",
       " 'aapso': 72,\n",
       " 'aar': 73,\n",
       " 'aarabi': 74,\n",
       " 'aardal': 75,\n",
       " 'aarhu': 76,\n",
       " 'aaron': 77,\n",
       " 'aaronson': 78,\n",
       " 'aaronsonwigderson': 79,\n",
       " 'aarr': 80,\n",
       " 'aarrow': 81,\n",
       " 'aart': 82,\n",
       " 'aarti': 83,\n",
       " 'aas': 84,\n",
       " 'aashtiani': 85,\n",
       " 'aastrategi': 86,\n",
       " 'aat': 87,\n",
       " 'aath': 88,\n",
       " 'aati': 89,\n",
       " 'aau': 90,\n",
       " 'aaw': 91,\n",
       " 'aawcondit': 92,\n",
       " 'aax': 93,\n",
       " 'aaxpi': 94,\n",
       " 'aazhang': 95,\n",
       " 'ab': 96,\n",
       " 'aba': 97,\n",
       " 'abaa': 98,\n",
       " 'abab': 99,\n",
       " 'ababa': 100,\n",
       " 'ababaaaaaaabaaaaabababaaaaab': 101,\n",
       " 'ababac': 102,\n",
       " 'ababc': 103,\n",
       " 'ababp': 104,\n",
       " 'abac': 105,\n",
       " 'abacaxi': 106,\n",
       " 'abacfromlog': 107,\n",
       " 'abad': 108,\n",
       " 'abadi': 109,\n",
       " 'abadirogaway': 110,\n",
       " 'abaioff': 111,\n",
       " 'abaixo': 112,\n",
       " 'abak': 113,\n",
       " 'abalon': 114,\n",
       " 'abandon': 115,\n",
       " 'abaqu': 116,\n",
       " 'abaquscaestandard': 117,\n",
       " 'abarbanel': 118,\n",
       " 'abarbu': 119,\n",
       " 'abas': 120,\n",
       " 'abat': 121,\n",
       " 'abath': 122,\n",
       " 'abatzogl': 123,\n",
       " 'abaz': 124,\n",
       " 'abb': 125,\n",
       " 'abba': 126,\n",
       " 'abbaa': 127,\n",
       " 'abbaac': 128,\n",
       " 'abbab': 129,\n",
       " 'abbadi': 130,\n",
       " 'abbasi': 131,\n",
       " 'abbasiyadkori': 132,\n",
       " 'abbass': 133,\n",
       " 'abbb': 134,\n",
       " 'abbbi': 135,\n",
       " 'abbeel': 136,\n",
       " 'abbey': 137,\n",
       " 'abbildung': 138,\n",
       " 'abbo': 139,\n",
       " 'abbott': 140,\n",
       " 'abboud': 141,\n",
       " 'abbrevi': 142,\n",
       " 'abc': 143,\n",
       " 'abca': 144,\n",
       " 'abcab': 145,\n",
       " 'abcabbcabc': 146,\n",
       " 'abcabc': 147,\n",
       " 'abcabcp': 148,\n",
       " 'abcacb': 149,\n",
       " 'abcba': 150,\n",
       " 'abcbac': 151,\n",
       " 'abcd': 152,\n",
       " 'abcda': 153,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 154,\n",
       " 'abcdi': 155,\n",
       " 'abcedighfnljmqrpovustywxz': 156,\n",
       " 'abcompress': 157,\n",
       " 'abcp': 158,\n",
       " 'abcw': 159,\n",
       " 'abcx': 160,\n",
       " 'abd': 161,\n",
       " 'abdallah': 162,\n",
       " 'abdceigfhjmlnqropstuvyzwx': 163,\n",
       " 'abdceigfhqropjm': 164,\n",
       " 'abdceigfhqropjmlnstuvyzwx': 165,\n",
       " 'abdeddam': 166,\n",
       " 'abdeg': 167,\n",
       " 'abdelberi': 168,\n",
       " 'abdelgawad': 169,\n",
       " 'abdelghaffar': 170,\n",
       " 'abdelhak': 171,\n",
       " 'abdelhakim': 172,\n",
       " 'abdelhaq': 173,\n",
       " 'abdelkad': 174,\n",
       " 'abdellatif': 175,\n",
       " 'abdelouahab': 176,\n",
       " 'abdelrahman': 177,\n",
       " 'abdelwahab': 178,\n",
       " 'abdelzah': 179,\n",
       " 'abdesselam': 180,\n",
       " 'abdolali': 181,\n",
       " 'abdoulay': 182,\n",
       " 'abduc': 183,\n",
       " 'abduct': 184,\n",
       " 'abdul': 185,\n",
       " 'abdula': 186,\n",
       " 'abdulkadiroglu': 187,\n",
       " 'abdulla': 188,\n",
       " 'abdullah': 189,\n",
       " 'abdullahalwadud': 190,\n",
       " 'abdulquad': 191,\n",
       " 'abdulrahman': 192,\n",
       " 'abdulsat': 193,\n",
       " 'abdur': 194,\n",
       " 'abdx': 195,\n",
       " 'abe': 196,\n",
       " 'abedmeraim': 197,\n",
       " 'abel': 198,\n",
       " 'abela': 199,\n",
       " 'abelfre': 200,\n",
       " 'abelian': 201,\n",
       " 'abello': 202,\n",
       " 'abelson': 203,\n",
       " 'abep': 204,\n",
       " 'aber': 205,\n",
       " 'abercrombi': 206,\n",
       " 'aberg': 207,\n",
       " 'abernethi': 208,\n",
       " 'aberth': 209,\n",
       " 'aberystwyth': 210,\n",
       " 'abeyesingh': 211,\n",
       " 'abfp': 212,\n",
       " 'abgrenzung': 213,\n",
       " 'abgrp': 214,\n",
       " 'abh': 215,\n",
       " 'abhandlungen': 216,\n",
       " 'abhay': 217,\n",
       " 'abhaya': 218,\n",
       " 'abhi': 219,\n",
       " 'abhijeet': 220,\n",
       " 'abhijit': 221,\n",
       " 'abhik': 222,\n",
       " 'abhinav': 223,\n",
       " 'abhiruk': 224,\n",
       " 'abhishek': 225,\n",
       " 'abhradeep': 226,\n",
       " 'abhyankar': 227,\n",
       " 'abi': 228,\n",
       " 'abid': 229,\n",
       " 'abigail': 230,\n",
       " 'abigqropfhdjst': 231,\n",
       " 'abigqropfhdjstuvmyzwxlnc': 232,\n",
       " 'abigqrpohfcednljvustmywxz': 233,\n",
       " 'abii': 234,\n",
       " 'abil': 235,\n",
       " 'abilitytyp': 236,\n",
       " 'abingdon': 237,\n",
       " 'abiresearch': 238,\n",
       " 'abishanka': 239,\n",
       " 'abiteboul': 240,\n",
       " 'abiwt': 241,\n",
       " 'abj': 242,\n",
       " 'abjad': 243,\n",
       " 'abk': 244,\n",
       " 'abl': 245,\n",
       " 'ablat': 246,\n",
       " 'ablayev': 247,\n",
       " 'ablenot': 248,\n",
       " 'abley': 249,\n",
       " 'abli': 250,\n",
       " 'ablmm': 251,\n",
       " 'ablog': 252,\n",
       " 'abm': 253,\n",
       " 'abmash': 254,\n",
       " 'abmashdoc': 255,\n",
       " 'abn': 256,\n",
       " 'abnorm': 257,\n",
       " 'abnormalfaulti': 258,\n",
       " 'abolhasan': 259,\n",
       " 'abolish': 260,\n",
       " 'abon': 261,\n",
       " 'abor': 262,\n",
       " 'aboratori': 263,\n",
       " 'abordagem': 264,\n",
       " 'abort': 265,\n",
       " 'abortship': 266,\n",
       " 'abouelleil': 267,\n",
       " 'aboulnasr': 268,\n",
       " 'abound': 269,\n",
       " 'aboutaw': 270,\n",
       " 'aboutorab': 271,\n",
       " 'aboutp': 272,\n",
       " 'aboutt': 273,\n",
       " 'aboveaverag': 274,\n",
       " 'abovebelow': 275,\n",
       " 'aboveclickablejqueri': 276,\n",
       " 'abovedefin': 277,\n",
       " 'abovedescrib': 278,\n",
       " 'abovediagon': 279,\n",
       " 'abovei': 280,\n",
       " 'abovement': 281,\n",
       " 'abovepdefinit': 282,\n",
       " 'abovepmean': 283,\n",
       " 'aboveproduc': 284,\n",
       " 'abovest': 285,\n",
       " 'abovethethreshold': 286,\n",
       " 'abovethreshold': 287,\n",
       " 'abovewhich': 288,\n",
       " 'abowd': 289,\n",
       " 'abox': 290,\n",
       " 'abp': 291,\n",
       " 'abpd': 292,\n",
       " 'abpl': 293,\n",
       " 'abpowerfre': 294,\n",
       " 'abpq': 295,\n",
       " 'abq': 296,\n",
       " 'abqp': 297,\n",
       " 'abr': 298,\n",
       " 'abraham': 299,\n",
       " 'abrahamson': 300,\n",
       " 'abram': 301,\n",
       " 'abramovich': 302,\n",
       " 'abramowitz': 303,\n",
       " 'abramski': 304,\n",
       " 'abramson': 305,\n",
       " 'abrego': 306,\n",
       " 'abreu': 307,\n",
       " 'abrial': 308,\n",
       " 'abridg': 309,\n",
       " 'abril': 310,\n",
       " 'abroad': 311,\n",
       " 'abrupt': 312,\n",
       " 'abruptli': 313,\n",
       " 'abrusci': 314,\n",
       " 'abrutyn': 315,\n",
       " 'abruzzi': 316,\n",
       " 'abs': 317,\n",
       " 'absastroph': 318,\n",
       " 'absb': 319,\n",
       " 'absc': 320,\n",
       " 'abschatzungen': 321,\n",
       " 'abscissa': 322,\n",
       " 'abscoeff': 323,\n",
       " 'abscondmat': 324,\n",
       " 'absenc': 325,\n",
       " 'absent': 326,\n",
       " 'absentthes': 327,\n",
       " 'absi': 328,\n",
       " 'absij': 329,\n",
       " 'absil': 330,\n",
       " 'absmathmg': 331,\n",
       " 'absolut': 332,\n",
       " 'absolutecontinu': 333,\n",
       " 'absolutecw': 334,\n",
       " 'absoluteleximin': 335,\n",
       " 'absoluteleximinbett': 336,\n",
       " 'absoluteleximinoptim': 337,\n",
       " 'absoluteleximinwelfar': 338,\n",
       " 'absolutelycontinu': 339,\n",
       " 'absolutelyrel': 340,\n",
       " 'absolutelyrich': 341,\n",
       " 'absolutelystrong': 342,\n",
       " 'absoluterel': 343,\n",
       " 'absoluterelativeleximinbett': 344,\n",
       " 'absoluterelativeleximinoptim': 345,\n",
       " 'absolutesquar': 346,\n",
       " 'absoluteutilitarian': 347,\n",
       " 'absolutevalu': 348,\n",
       " 'absolutewmaxim': 349,\n",
       " 'absolutewp': 350,\n",
       " 'absorb': 351,\n",
       " 'absorpt': 352,\n",
       " 'absp': 353,\n",
       " 'absquantph': 354,\n",
       " 'abstent': 355,\n",
       " 'abstract': 356,\n",
       " 'abstracta': 357,\n",
       " 'abstractaccur': 358,\n",
       " 'abstractalthough': 359,\n",
       " 'abstractanomali': 360,\n",
       " 'abstractapproxim': 361,\n",
       " 'abstractchaot': 362,\n",
       " 'abstractcomplex': 363,\n",
       " 'abstractdesign': 364,\n",
       " 'abstractdetect': 365,\n",
       " 'abstractdifferenti': 366,\n",
       " 'abstractdu': 367,\n",
       " 'abstractemot': 368,\n",
       " 'abstractergod': 369,\n",
       " 'abstractestim': 370,\n",
       " 'abstractfollow': 371,\n",
       " 'abstractfrequ': 372,\n",
       " 'abstractgiven': 373,\n",
       " 'abstractin': 374,\n",
       " 'abstractinformationcontrolcr': 375,\n",
       " 'abstractioninform': 376,\n",
       " 'abstractionrefin': 377,\n",
       " 'abstractit': 378,\n",
       " 'abstractli': 379,\n",
       " 'abstractlinkedlist': 380,\n",
       " 'abstractlinkedlistinit': 381,\n",
       " 'abstractlinkedlistisequalvalu': 382,\n",
       " 'abstractloc': 383,\n",
       " 'abstractmanga': 384,\n",
       " 'abstractmani': 385,\n",
       " 'abstractmodern': 386,\n",
       " 'abstractmodular': 387,\n",
       " 'abstractmotiv': 388,\n",
       " 'abstractmultiarm': 389,\n",
       " 'abstractnetwork': 390,\n",
       " 'abstractneur': 391,\n",
       " 'abstractnumer': 392,\n",
       " 'abstractorderedmapdecoratorabstractorderedmapdecor': 393,\n",
       " 'abstractpast': 394,\n",
       " 'abstractpatchbas': 395,\n",
       " 'abstractposit': 396,\n",
       " 'abstractprovid': 397,\n",
       " 'abstractscholarli': 398,\n",
       " 'abstractsearch': 399,\n",
       " 'abstractsensor': 400,\n",
       " 'abstractsequenti': 401,\n",
       " 'abstractsimplici': 402,\n",
       " 'abstractsimultan': 403,\n",
       " 'abstractspars': 404,\n",
       " 'abstractst': 405,\n",
       " 'abstractstealthi': 406,\n",
       " 'abstractth': 407,\n",
       " 'abstractther': 408,\n",
       " 'abstractthi': 409,\n",
       " 'abstractto': 410,\n",
       " 'abstracttoward': 411,\n",
       " 'abstracttwohop': 412,\n",
       " 'abstractvehicl': 413,\n",
       " 'abstractw': 414,\n",
       " 'abstractwith': 415,\n",
       " 'abstrus': 416,\n",
       " 'absurd': 417,\n",
       " 'absurdum': 418,\n",
       " 'absv': 419,\n",
       " 'absw': 420,\n",
       " 'absz': 421,\n",
       " 'abt': 422,\n",
       " 'abtahi': 423,\n",
       " 'abu': 424,\n",
       " 'abualrub': 425,\n",
       " 'abubakr': 426,\n",
       " 'abugida': 427,\n",
       " 'abujarad': 428,\n",
       " 'abund': 429,\n",
       " 'abundanceecosystem': 430,\n",
       " 'abundanceproject': 431,\n",
       " 'abundantdata': 432,\n",
       " 'abundantli': 433,\n",
       " 'abuniform': 434,\n",
       " 'abunimeh': 435,\n",
       " 'abuot': 436,\n",
       " 'abur': 437,\n",
       " 'abus': 438,\n",
       " 'abusefre': 439,\n",
       " 'abuseof': 440,\n",
       " 'abusufah': 441,\n",
       " 'abut': 442,\n",
       " 'abuv': 443,\n",
       " 'abuz': 444,\n",
       " 'abuzaid': 445,\n",
       " 'abv': 446,\n",
       " 'abw': 447,\n",
       " 'abx': 448,\n",
       " 'abxi': 449,\n",
       " 'abxt': 450,\n",
       " 'abya': 451,\n",
       " 'abz': 452,\n",
       " 'ac': 453,\n",
       " 'aca': 454,\n",
       " 'acad': 455,\n",
       " 'academ': 456,\n",
       " 'academi': 457,\n",
       " 'academia': 458,\n",
       " 'academiaedu': 459,\n",
       " 'academiai': 460,\n",
       " 'academician': 461,\n",
       " 'academicplenum': 462,\n",
       " 'acadsci': 463,\n",
       " 'acalgorithm': 464,\n",
       " 'acampora': 465,\n",
       " 'acanon': 466,\n",
       " 'acant': 467,\n",
       " 'acapulco': 468,\n",
       " 'acar': 469,\n",
       " 'acarchau': 470,\n",
       " 'acasteigtsjbotsim': 471,\n",
       " 'acaus': 472,\n",
       " 'acb': 473,\n",
       " 'acbab': 474,\n",
       " 'acbc': 475,\n",
       " 'acbct': 476,\n",
       " 'acbd': 477,\n",
       " 'acbt': 478,\n",
       " 'acc': 479,\n",
       " 'accademia': 480,\n",
       " 'accardi': 481,\n",
       " 'accdf': 482,\n",
       " 'accdg': 483,\n",
       " 'accdhyp': 484,\n",
       " 'accdm': 485,\n",
       " 'accel': 486,\n",
       " 'acceler': 487,\n",
       " 'acceleratedparallel': 488,\n",
       " 'accelerationaid': 489,\n",
       " 'acceleromet': 490,\n",
       " 'accent': 491,\n",
       " 'accentu': 492,\n",
       " 'accentur': 493,\n",
       " 'accept': 494,\n",
       " 'acceptancereject': 495,\n",
       " 'acceptingfin': 496,\n",
       " 'acceptk': 497,\n",
       " 'acceptor': 498,\n",
       " 'acceptreject': 499,\n",
       " 'acceptsrejectsrestart': 500,\n",
       " 'acces': 501,\n",
       " 'access': 502,\n",
       " 'accesscognit': 503,\n",
       " 'accesscontrol': 504,\n",
       " 'accessedmodifi': 505,\n",
       " 'accessiblilti': 506,\n",
       " 'accessor': 507,\n",
       " 'accessori': 508,\n",
       " 'accf': 509,\n",
       " 'accg': 510,\n",
       " 'accghyp': 511,\n",
       " 'acchyp': 512,\n",
       " 'acci': 513,\n",
       " 'accid': 514,\n",
       " 'accident': 515,\n",
       " 'accl': 516,\n",
       " 'acclaim': 517,\n",
       " 'accm': 518,\n",
       " 'accn': 519,\n",
       " 'acco': 520,\n",
       " 'accomazzi': 521,\n",
       " 'accommod': 522,\n",
       " 'accomod': 523,\n",
       " 'accompani': 524,\n",
       " 'accomplic': 525,\n",
       " 'accomplish': 526,\n",
       " 'acconnectedsubset': 527,\n",
       " 'accor': 528,\n",
       " 'accord': 529,\n",
       " 'accordi': 530,\n",
       " 'accordingli': 531,\n",
       " 'account': 532,\n",
       " 'accountident': 533,\n",
       " 'accountmanag': 534,\n",
       " 'accountmanagerservic': 535,\n",
       " 'accoust': 536,\n",
       " 'accratef': 537,\n",
       " 'accratefhyp': 538,\n",
       " 'accrateg': 539,\n",
       " 'accrateghyp': 540,\n",
       " 'accratem': 541,\n",
       " 'accrel': 542,\n",
       " 'accru': 543,\n",
       " 'acct': 544,\n",
       " 'accuari': 545,\n",
       " 'accumul': 546,\n",
       " 'accur': 547,\n",
       " 'accuraci': 548,\n",
       " 'accuraciesfscor': 549,\n",
       " 'accuracyfavor': 550,\n",
       " 'accuracyfscor': 551,\n",
       " 'accuracyit': 552,\n",
       " 'accuracymemori': 553,\n",
       " 'accuracyp': 554,\n",
       " 'accuracyprecis': 555,\n",
       " 'accuracysolut': 556,\n",
       " 'accuracyspecif': 557,\n",
       " 'accuracyth': 558,\n",
       " 'accuracyther': 559,\n",
       " 'accus': 560,\n",
       " 'accv': 561,\n",
       " 'accx': 562,\n",
       " 'acd': 563,\n",
       " 'acda': 564,\n",
       " 'acdar': 565,\n",
       " 'acdau': 566,\n",
       " 'acdb': 567,\n",
       " 'ace': 568,\n",
       " 'acea': 569,\n",
       " 'acebr': 570,\n",
       " 'acebron': 571,\n",
       " 'acecomplet': 572,\n",
       " 'acehard': 573,\n",
       " 'acehigh': 574,\n",
       " 'acemoglu': 575,\n",
       " 'acentr': 576,\n",
       " 'acerbi': 577,\n",
       " 'acerca': 578,\n",
       " 'acero': 579,\n",
       " 'aceska': 580,\n",
       " 'aceto': 581,\n",
       " 'acevedorocha': 582,\n",
       " 'acf': 583,\n",
       " 'acfeas': 584,\n",
       " 'acfor': 585,\n",
       " 'acg': 586,\n",
       " 'acgilbert': 587,\n",
       " 'acgt': 588,\n",
       " 'ach': 589,\n",
       " 'achain': 590,\n",
       " 'achan': 591,\n",
       " 'achanta': 592,\n",
       " 'achar': 593,\n",
       " 'achara': 594,\n",
       " 'achari': 595,\n",
       " 'acharya': 596,\n",
       " 'acheiev': 597,\n",
       " 'achi': 598,\n",
       " 'achieiv': 599,\n",
       " 'achiev': 600,\n",
       " 'achievabil': 601,\n",
       " 'achievabiliy': 602,\n",
       " 'achievabl': 603,\n",
       " 'achievebl': 604,\n",
       " 'achievedpsfrag': 605,\n",
       " 'achil': 606,\n",
       " 'achillea': 607,\n",
       " 'achilleo': 608,\n",
       " 'achim': 609,\n",
       " 'achin': 610,\n",
       " 'achiv': 611,\n",
       " 'achliopta': 612,\n",
       " 'achrekar': 613,\n",
       " 'achterberg': 614,\n",
       " 'achtergracht': 615,\n",
       " 'aci': 616,\n",
       " 'acial': 617,\n",
       " 'acid': 618,\n",
       " 'acifar': 619,\n",
       " 'acij': 620,\n",
       " 'acin': 621,\n",
       " 'acit': 622,\n",
       " 'aciv': 623,\n",
       " 'acjp': 624,\n",
       " 'ack': 625,\n",
       " 'ackbas': 626,\n",
       " 'ackerman': 627,\n",
       " 'ackermann': 628,\n",
       " 'acketswitch': 629,\n",
       " 'ackfact': 630,\n",
       " 'ackijk': 631,\n",
       " 'ackley': 632,\n",
       " 'acklm': 633,\n",
       " 'acknowledg': 634,\n",
       " 'acknowledgementsw': 635,\n",
       " 'acknowledgmentbas': 636,\n",
       " 'acknowlegd': 637,\n",
       " 'ackoff': 638,\n",
       " 'ackowledg': 639,\n",
       " 'ackstrom': 640,\n",
       " 'ackt': 641,\n",
       " 'acl': 642,\n",
       " 'aclc': 643,\n",
       " 'aclcol': 644,\n",
       " 'aclf': 645,\n",
       " 'aclijcnlp': 646,\n",
       " 'aclremoveentri': 647,\n",
       " 'aclust': 648,\n",
       " 'acm': 649,\n",
       " 'acmec': 650,\n",
       " 'acmgi': 651,\n",
       " 'acmicpc': 652,\n",
       " 'acmiee': 653,\n",
       " 'acmifipusenix': 654,\n",
       " 'acml': 655,\n",
       " 'acmmm': 656,\n",
       " 'acmsiam': 657,\n",
       " 'acmsigda': 658,\n",
       " 'acmsigmod': 659,\n",
       " 'acn': 660,\n",
       " 'acnc': 661,\n",
       " 'acnd': 662,\n",
       " 'aco': 663,\n",
       " 'acod': 664,\n",
       " 'acoeffici': 665,\n",
       " 'acohido': 666,\n",
       " 'acolor': 667,\n",
       " 'acom': 668,\n",
       " 'acompi': 669,\n",
       " 'acompon': 670,\n",
       " 'acomprehens': 671,\n",
       " 'acomput': 672,\n",
       " 'aconnect': 673,\n",
       " 'aconsist': 674,\n",
       " 'aconst': 675,\n",
       " 'acoord': 676,\n",
       " 'acopf': 677,\n",
       " 'acori': 678,\n",
       " 'acorrect': 679,\n",
       " 'acosd': 680,\n",
       " 'acostaleit': 681,\n",
       " 'acou': 682,\n",
       " 'acount': 683,\n",
       " 'acoust': 684,\n",
       " 'acp': 685,\n",
       " 'acpa': 686,\n",
       " 'acpf': 687,\n",
       " 'acq': 688,\n",
       " 'acqpa': 689,\n",
       " 'acquaint': 690,\n",
       " 'acquir': 691,\n",
       " 'acquisit': 692,\n",
       " 'acquisti': 693,\n",
       " 'acr': 694,\n",
       " 'acrf': 695,\n",
       " 'acri': 696,\n",
       " 'acro': 697,\n",
       " 'acrocel': 698,\n",
       " 'acronym': 699,\n",
       " 'across': 700,\n",
       " 'acrosscategori': 701,\n",
       " 'acrossdataset': 702,\n",
       " 'acrosstheboard': 703,\n",
       " 'acrow': 704,\n",
       " 'acsac': 705,\n",
       " 'acscastelluccia': 706,\n",
       " 'acsd': 707,\n",
       " 'acssc': 708,\n",
       " 'acsysimp': 709,\n",
       " 'act': 710,\n",
       " 'acta': 711,\n",
       " 'acteris': 712,\n",
       " 'acti': 713,\n",
       " 'actin': 714,\n",
       " 'action': 715,\n",
       " 'actioncomedydrama': 716,\n",
       " 'actiondata': 717,\n",
       " 'actionev': 718,\n",
       " 'actionif': 719,\n",
       " 'actionimpuls': 720,\n",
       " 'actionlabel': 721,\n",
       " 'actionpageid': 722,\n",
       " 'actionpercept': 723,\n",
       " 'actionplay': 724,\n",
       " 'actionpolici': 725,\n",
       " 'actionrecord': 726,\n",
       " 'actionsth': 727,\n",
       " 'actionstreat': 728,\n",
       " 'actiontreat': 729,\n",
       " 'activ': 730,\n",
       " 'activandrewcmueduuserbhooiratingstar': 731,\n",
       " 'activationbas': 732,\n",
       " 'activationdescactivationdesc': 733,\n",
       " 'activationgroup': 734,\n",
       " 'activationgroupcreategroup': 735,\n",
       " 'activationgroupinactiveobject': 736,\n",
       " 'activationinactiv': 737,\n",
       " 'activationsystem': 738,\n",
       " 'activeadapt': 739,\n",
       " 'activedomain': 740,\n",
       " 'activepass': 741,\n",
       " 'activereact': 742,\n",
       " 'activeset': 743,\n",
       " 'activesetlik': 744,\n",
       " 'activestrategi': 745,\n",
       " 'activist': 746,\n",
       " 'activit': 747,\n",
       " 'activityactonto': 748,\n",
       " 'activityaspect': 749,\n",
       " 'activityawar': 750,\n",
       " 'activitybas': 751,\n",
       " 'activityedg': 752,\n",
       " 'activityedgeaspect': 753,\n",
       " 'activityedgeimpl': 754,\n",
       " 'activityfactor': 755,\n",
       " 'activityimpl': 756,\n",
       " 'activitynet': 757,\n",
       " 'activitynod': 758,\n",
       " 'activitynodeactivationgroup': 759,\n",
       " 'activitynodeaspect': 760,\n",
       " 'activitynodeimpl': 761,\n",
       " 'activityparameternod': 762,\n",
       " 'activityserviceproviderreceiv': 763,\n",
       " 'activitythat': 764,\n",
       " 'activitytravel': 765,\n",
       " 'activityui': 766,\n",
       " 'activityus': 767,\n",
       " 'activityx': 768,\n",
       " 'actno': 769,\n",
       " 'acton': 770,\n",
       " 'actonto': 771,\n",
       " 'actor': 772,\n",
       " 'actorbas': 773,\n",
       " 'actorbehavior': 774,\n",
       " 'actorcount': 775,\n",
       " 'actorcrit': 776,\n",
       " 'actorfor': 777,\n",
       " 'actorfoundri': 778,\n",
       " 'actorfoundrywhich': 779,\n",
       " 'actorinspir': 780,\n",
       " 'actorpoint': 781,\n",
       " 'actorr': 782,\n",
       " 'actorreact': 783,\n",
       " 'actorref': 784,\n",
       " 'actorrepli': 785,\n",
       " 'actorsbas': 786,\n",
       " 'actorsthousand': 787,\n",
       " 'actoutcom': 788,\n",
       " 'actpbq': 789,\n",
       " 'actpq': 790,\n",
       " 'actpqq': 791,\n",
       " 'actq': 792,\n",
       " 'actr': 793,\n",
       " 'actscr': 794,\n",
       " 'actsh': 795,\n",
       " 'actshr': 796,\n",
       " 'actsi': 797,\n",
       " 'actslo': 798,\n",
       " 'actt': 799,\n",
       " 'actu': 800,\n",
       " 'actual': 801,\n",
       " 'actualis': 802,\n",
       " 'actualment': 803,\n",
       " 'actuari': 804,\n",
       " 'actuat': 805,\n",
       " 'acu': 806,\n",
       " 'acukcjbimagesgfhtml': 807,\n",
       " 'acustica': 808,\n",
       " 'acut': 809,\n",
       " 'acvt': 810,\n",
       " 'acx': 811,\n",
       " 'acycl': 812,\n",
       " 'acyclicli': 813,\n",
       " 'acycloid': 814,\n",
       " 'acyl': 815,\n",
       " 'aczel': 816,\n",
       " 'ad': 817,\n",
       " 'ada': 818,\n",
       " 'adaba': 819,\n",
       " 'adaboost': 820,\n",
       " 'adachi': 821,\n",
       " 'adadelta': 822,\n",
       " 'adadi': 823,\n",
       " 'adag': 824,\n",
       " 'adak': 825,\n",
       " 'adalbert': 826,\n",
       " 'adalf': 827,\n",
       " 'adalfsdk': 828,\n",
       " 'adalfsdt': 829,\n",
       " 'adalsteinsson': 830,\n",
       " 'adam': 831,\n",
       " 'adamatzki': 832,\n",
       " 'adamczak': 833,\n",
       " 'adamek': 834,\n",
       " 'adami': 835,\n",
       " 'adamicadar': 836,\n",
       " 'adamof': 837,\n",
       " 'adamsbashforth': 838,\n",
       " 'adamsbashforthmoulton': 839,\n",
       " 'adamsmoulton': 840,\n",
       " 'adamson': 841,\n",
       " 'adao': 842,\n",
       " 'adap': 843,\n",
       " 'adapt': 844,\n",
       " 'adaptao': 845,\n",
       " 'adaptationgfk': 846,\n",
       " 'adaptivediscovershar': 847,\n",
       " 'adaptivelychosen': 848,\n",
       " 'adaptivelysecur': 849,\n",
       " 'adaptivemax': 850,\n",
       " 'adaptivemean': 851,\n",
       " 'adaptiverobust': 852,\n",
       " 'adaptiveselect': 853,\n",
       " 'adaptivesum': 854,\n",
       " 'adaptivetre': 855,\n",
       " 'adar': 856,\n",
       " 'adaricheva': 857,\n",
       " 'adat': 858,\n",
       " 'adatadriveb': 859,\n",
       " 'adavani': 860,\n",
       " 'adawar': 861,\n",
       " 'adbas': 862,\n",
       " 'adbbb': 863,\n",
       " 'adc': 864,\n",
       " 'adcock': 865,\n",
       " 'adcol': 866,\n",
       " 'add': 867,\n",
       " 'addabbo': 868,\n",
       " 'addaiu': 869,\n",
       " 'addarioberri': 870,\n",
       " 'addcandrul': 871,\n",
       " 'addcandrulesu': 872,\n",
       " 'addcandruleu': 873,\n",
       " 'addclocklisten': 874,\n",
       " 'adddit': 875,\n",
       " 'adddoubl': 876,\n",
       " 'adddoubleaddr': 877,\n",
       " 'addednot': 878,\n",
       " 'addedremov': 879,\n",
       " 'addedsubtract': 880,\n",
       " 'addend': 881,\n",
       " 'addendum': 882,\n",
       " 'adder': 883,\n",
       " 'addet': 884,\n",
       " 'addflip': 885,\n",
       " 'addfloat': 886,\n",
       " 'addfloataddr': 887,\n",
       " 'addhalf': 888,\n",
       " 'addi': 889,\n",
       " 'addict': 890,\n",
       " 'addingin': 891,\n",
       " 'addingremov': 892,\n",
       " 'addingsubtract': 893,\n",
       " 'addint': 894,\n",
       " 'addintaddr': 895,\n",
       " 'addintlit': 896,\n",
       " 'addisionwesley': 897,\n",
       " 'addison': 898,\n",
       " 'addisonweasli': 899,\n",
       " 'addisonwesley': 900,\n",
       " 'addisson': 901,\n",
       " 'addissonwesley': 902,\n",
       " 'addit': 903,\n",
       " 'additio': 904,\n",
       " 'additiondelet': 905,\n",
       " 'additionfor': 906,\n",
       " 'additionn': 907,\n",
       " 'additionremov': 908,\n",
       " 'additiveacrossapartit': 909,\n",
       " 'additiveerror': 910,\n",
       " 'additivelyidempot': 911,\n",
       " 'additivemenus': 912,\n",
       " 'additivemenusizem': 913,\n",
       " 'additiven': 914,\n",
       " 'additivepfor': 915,\n",
       " 'addiu': 916,\n",
       " 'addlong': 917,\n",
       " 'addlongaddr': 918,\n",
       " 'addnod': 919,\n",
       " 'addon': 920,\n",
       " 'addpositioningconstraint': 921,\n",
       " 'addpx': 922,\n",
       " 'addr': 923,\n",
       " 'addrandomappl': 924,\n",
       " 'address': 925,\n",
       " 'addressbook': 926,\n",
       " 'addresse': 927,\n",
       " 'addressedand': 928,\n",
       " 'addscor': 929,\n",
       " 'addscorei': 930,\n",
       " 'addtion': 931,\n",
       " 'addu': 932,\n",
       " 'adduc': 933,\n",
       " 'addx': 934,\n",
       " 'adec': 935,\n",
       " 'adecod': 936,\n",
       " 'adegre': 937,\n",
       " 'adel': 938,\n",
       " 'adelaid': 939,\n",
       " 'adelghaffar': 940,\n",
       " 'adelgren': 941,\n",
       " 'adelin': 942,\n",
       " 'adelmanmccarthi': 943,\n",
       " 'adelmann': 944,\n",
       " 'adelson': 945,\n",
       " 'aden': 946,\n",
       " 'adenin': 947,\n",
       " 'adequ': 948,\n",
       " 'adequaci': 949,\n",
       " 'aderdg': 950,\n",
       " 'aderiv': 951,\n",
       " 'adesnik': 952,\n",
       " 'adesso': 953,\n",
       " 'adet': 954,\n",
       " 'adetermin': 955,\n",
       " 'adetunji': 956,\n",
       " 'adewol': 957,\n",
       " 'adford': 958,\n",
       " 'adfsdca': 959,\n",
       " 'adg': 960,\n",
       " 'adga': 961,\n",
       " 'adgarecogniz': 962,\n",
       " 'adh': 963,\n",
       " 'adham': 964,\n",
       " 'adher': 965,\n",
       " 'adhes': 966,\n",
       " 'adhesionrepuls': 967,\n",
       " 'adhikari': 968,\n",
       " 'adhoc': 969,\n",
       " 'adhocpublizit': 970,\n",
       " 'adhu': 971,\n",
       " 'adi': 972,\n",
       " 'adiabat': 973,\n",
       " 'adiac': 974,\n",
       " 'adibi': 975,\n",
       " 'adic': 976,\n",
       " 'adida': 977,\n",
       " 'adifor': 978,\n",
       " 'adigit': 979,\n",
       " 'adihedr': 980,\n",
       " 'adijac': 981,\n",
       " 'adijacautomat': 982,\n",
       " 'adilson': 983,\n",
       " 'adim': 984,\n",
       " 'adimat': 985,\n",
       " 'adinfinitum': 986,\n",
       " 'adio': 987,\n",
       " 'adipos': 988,\n",
       " 'adirect': 989,\n",
       " 'adish': 990,\n",
       " 'adist': 991,\n",
       " 'adistm': 992,\n",
       " 'adit': 993,\n",
       " 'aditya': 994,\n",
       " 'adiwena': 995,\n",
       " 'adj': 996,\n",
       " 'adjac': 997,\n",
       " 'adjacencypreserv': 998,\n",
       " 'adjacent': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'w') as f:\n",
    "    for item in vectorizer.get_feature_names():\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'BoW1.sav'\n",
    "pickle.dump(vectorizer, open(filename, 'wb'))\n",
    "#loaded_model = pickle.load(open('BoW1.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(vocabulary - words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(conda)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
